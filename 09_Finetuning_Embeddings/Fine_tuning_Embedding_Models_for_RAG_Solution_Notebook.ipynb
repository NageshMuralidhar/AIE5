{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbj5diaHkg"
      },
      "source": [
        "# Fine-tuning Embeddings for RAG on Specific Data\n",
        "\n",
        "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
        "\n",
        "Fine-tuning embeddings!\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  - Task 1: Dependencies and Boilerplate\n",
        "  - Task 2: Loading Data\n",
        "  - Task 3: Constructing a Fine-tuning Dataset\n",
        "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "  - Task 5: Evaluating our Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwor_3X6ODX"
      },
      "source": [
        "#### Basic Overview of Fine-tuning Embeddings\n",
        "\n",
        "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
        "\n",
        "```\n",
        "Move the embeddings for questions relating to a document\n",
        "closer together with that document\n",
        "```\n",
        "\n",
        "We can think of fine-tuning our embedding models as follows:\n",
        "\n",
        "1) We have some pair of text items that *should* be closer together\n",
        "  - `Question`, `Document` pairs\n",
        "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
        "\n",
        "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
        "\n",
        "The process of training helps the model more accurately associate our questions with the correct documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5R3HVz6FOQ"
      },
      "source": [
        "#####‚ùì Question #1:\n",
        "\n",
        "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
        "\n",
        "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "We are specifically relating *the questions* to *the documents*. This means that we are making our embedding model at the very specific task of relating potential questions to specific documents.\n",
        "\n",
        "There are many caveats, but the main ones are:\n",
        "\n",
        "- Your Q's should reflect the Q's of your users\n",
        "- This kind of fine-tuning will (purposefully) \"overfit\" on your data; this is the desired result in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NkSaurzbpyS"
      },
      "source": [
        "## Task 1: Dependencies and Boilerplate\n",
        "\n",
        "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
        "\n",
        "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_EUibmcDU3"
      },
      "source": [
        "### Nest Asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zq-6s7LbPnKH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8uFz8RVcFFu"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        ">> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulZIBA1ZoSsV",
        "outputId": "12d9c766-843f-40bf-bdf8-e0ed04b6d87f"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3GFD7B-tOCrx"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FM-eUlrcI8a"
      },
      "source": [
        "### Provide OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_mlurVqtrp",
        "outputId": "18cccb1e-095f-40fa-def5-2454f9bcdcae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ217gCDVTr"
      },
      "source": [
        "## Task 2: Loading Data\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‚Äòdata‚Äô: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31440    0 31440    0     0   124k      0 --:--:-- --:--:-- --:--:--  124k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 70299    0 70299    0     0   655k      0 --:--:-- --:--:-- --:--:--  660k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DHJhTzsvN75t"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "path = \"data/\"\n",
        "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbKa6-V0nvp"
      },
      "source": [
        "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NsPrOOqXOsNX"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_PoX7l09Rg"
      },
      "source": [
        "Next we can load/split these documents as follows.\n",
        "\n",
        ">> NOTE: You may need to run this cell twice to get it to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OMYPX6N6Os8M"
      },
      "outputs": [],
      "source": [
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAozuMoNOvnp",
        "outputId": "dc1d663e-7153-4c51-cedb-d1bc3888c4ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yE2TFIq1BuJ"
      },
      "source": [
        "Next, we're going to associate each of our chunks with a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AwyIForybIpo"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "id_set = set()\n",
        "\n",
        "for document in training_documents:\n",
        "  id = str(uuid.uuid4())\n",
        "  while id in id_set:\n",
        "    id = uuid.uuid4()\n",
        "  id_set.add(id)\n",
        "  document.metadata[\"id\"] = id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '0f046e40-6ca1-4088-b332-4f88e3dd3def'}, page_content='Stuff we figured out about AI in 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSimon Willison‚Äôs Weblog\\nSubscribe\\n\\n\\n\\n\\n\\n\\nStuff we figured out about AI in 2023\\n31st December 2023\\n2023 was the breakthrough year for Large Language Models (LLMs). I think it‚Äôs OK to call these AI‚Äîthey‚Äôre the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\\nHere‚Äôs my attempt to round up the highlights in one place!'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'be5cfe3a-20dc-46a3-890d-969bd8cf7c5f'}, page_content='Large Language Models\\nThey‚Äôre actually quite easy to build\\nYou can run LLMs on your own devices\\nHobbyists can build their own fine-tuned models\\nWe don‚Äôt yet know how to build GPT-4\\nVibes Based Development\\nLLMs are really smart, and also really, really dumb\\nGullibility is the biggest unsolved problem\\nCode may be the best application\\nThe ethics of this space remain diabolically complex\\nMy blog in 2023'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'ec2e3da4-8456-4d51-94b1-054c6645df4b'}, page_content='Here‚Äôs the sequel to this post: Things we learned about LLMs in 2024.\\nLarge Language Models\\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '4c77812b-94e3-417f-9bee-d72830e179f2'}, page_content='So far, I think they‚Äôre a net positive. I‚Äôve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\\nThey‚Äôre actually quite easy to build\\nThe most surprising thing we‚Äôve learned about LLMs this year is that they‚Äôre actually quite easy to build.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'f6b632e7-a106-4482-b01c-2dfced38a68d'}, page_content='Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'c4902f3e-784d-44f8-bf27-9c32f9954df9'}, page_content='A year ago, the only organization that had released a generally useful LLM was OpenAI. We‚Äôve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\\nThe training cost (hardware and electricity) is still significant‚Äîinitially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft‚Äôs Phi-2 claims to have used ‚Äú14 days on 96 A100 GPUs‚Äù, which works out at around $35,000 using current Lambda pricing.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'd4f2e7bc-58a5-417f-9883-e258e9398bb4'}, page_content='So training an LLM still isn‚Äôt something a hobbyist can afford, but it‚Äôs no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge‚Äînot trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia‚Äôs Suspension bridges by country category lists 44 countries).\\nYou can run LLMs on your own devices\\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '23280420-bd9f-4cdc-ae8f-8d12cdd0c289'}, page_content='Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2‚Äîan improved version which, crucially, included permission for commercial use.\\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'db344aee-6eec-4397-9f8c-e890510bc2e5'}, page_content='I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\\nHobbyists can build their own fine-tuned models\\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'eabbae1f-e665-47b2-85d3-dcd34a7d1088'}, page_content='There‚Äôs now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can‚Äôt even attempt to count them, and any count would be out-of-date within a few hours.\\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it‚Äôs whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\\nThis is a huge advantage for open over closed models: the closed, hosted models don‚Äôt have thousands of researchers and hobbyists around the world collaborating and competing to improve them.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'ab664489-9013-4a65-8ce8-fe9c745fc4a6'}, page_content='We don‚Äôt yet know how to build GPT-4\\nFrustratingly, despite the enormous leaps ahead we‚Äôve had this year, we are yet to see an alternative model that‚Äôs better than GPT-4.\\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\\nThis may well change in the next few weeks: Google‚Äôs Gemini Ultra has big claims, but isn‚Äôt yet available for us to try out.\\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they‚Äôve released two significant improvements since then.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '58e136a3-834b-49d2-b08e-6329c9a89cae'}, page_content='Still, I‚Äôm surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven‚Äôt shared yet.\\nVibes Based Development\\nAs a computer scientist and software engineer, LLMs are infuriating.\\nEven the openly licensed ones are still the world‚Äôs most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\\nI‚Äôm used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\\nThe worst part is the challenge of evaluating them.\\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually ‚Äúfeels‚Äù right when you try it for a given task.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'a02e8912-cee0-494a-af70-c75c756eeaef'}, page_content='I find I have to work with an LLM for a few weeks in order to get a good intuition for it‚Äôs strengths and weaknesses. This greatly limits how many I can evaluate myself!\\nThe most frustrating thing for me is at the level of individual prompting.\\nSometimes I‚Äôll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don‚Äôt have a good methodology for figuring that out.\\nWe‚Äôre left with what‚Äôs effectively Vibes Based Development. It‚Äôs vibes all the way down.\\nI‚Äôd love to see us move beyond vibes in 2024!\\nLLMs are really smart, and also really, really dumb'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '5cb6c17c-e65d-4486-a857-34381039b470'}, page_content='On the one hand, we keep on finding new things that LLMs can do that we didn‚Äôt expect‚Äîand that the people who trained the models didn‚Äôt expect either. That‚Äôs usually really fun!\\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\\nThe honest answer is ‚Äúmaybe‚Äù! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '6891002e-f5d4-4b4f-893e-c0e4da6be3fd'}, page_content='Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can‚Äôt type because you don‚Äôt have any fingers it produces the full code for you instead.\\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It‚Äôs all so dumb, but it works!\\nGullibility is the biggest unsolved problem\\nI coined the term prompt injection in September last year.\\n15 months later, I regret to say that we‚Äôre still no closer to a robust, dependable solution to this problem.\\nI‚Äôve written a ton about this already.\\nBeyond that specific class of security vulnerabilities, I‚Äôve started seeing this as a wider problem of gullibility.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '5c0fa430-70dd-4845-9b00-c762087df395'}, page_content='Language Models are gullible. They ‚Äúbelieve‚Äù what we tell them‚Äîwhat‚Äôs in their training data, then what‚Äôs in the fine-tuning data, then what‚Äôs in the prompt.\\nIn order to be useful tools for us, we need them to believe what we feed them!\\nBut it turns out a lot of the things we want to build need them not to be gullible.\\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '00ffd157-f4de-40aa-8a04-df7064408559'}, page_content='A lot of people are excited about AI agents‚Äîan infuriatingly vague term that seems to be converging on ‚ÄúAI systems that can go away and act on your behalf‚Äù. We‚Äôve been talking about them all year, but I‚Äôve seen few if any examples of them running in production, despite lots of exciting prototypes.\\nI think this is because of gullibility.\\nCan we solve this? Honestly, I‚Äôm beginning to suspect that you can‚Äôt fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\\nCode may be the best application\\nOver the course of the year, it‚Äôs become increasingly clear that writing code is one of the things LLMs are most capable of.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'c5a6fb98-4f30-4fe4-a8c2-5ccc22516854'}, page_content='If you think about what they do, this isn‚Äôt such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\\nIt‚Äôs still astonishing to me how effective they are though.\\nOne of the great weaknesses of LLMs is their tendency to hallucinate‚Äîto imagine things that don‚Äôt correspond to reality. You would expect this to be a particularly bad problem for code‚Äîif an LLM hallucinates a method that doesn‚Äôt exist, the code should be useless.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '399d8d58-0014-4c70-973d-7b850ff596b3'}, page_content='Except... you can run generated code to see if it‚Äôs correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\\nHow should we feel about this as software engineers?\\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '3b64aa30-86aa-427b-83b5-9288cc69233e'}, page_content='On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We‚Äôve all been given weird coding interns‚Äîwe can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\\nThe ethics of this space remain diabolically complex\\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'f1ea9027-3bc4-4901-9609-55ef134f200e'}, page_content='Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading‚Äîespecially the first few pages, which lay out the issues in a way that‚Äôs surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I‚Äôve read anywhere.\\nThe legal arguments here are complex. I‚Äôm not a lawyer, but I don‚Äôt think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '8854496a-da0a-46a7-ac0d-baff4b37945d'}, page_content='Law is not ethics. Is it OK to train models on people‚Äôs content without their permission, when those models will then be used in ways that compete with those people?\\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\\nPeople have certainly lost work to them‚Äîanecdotally, I‚Äôve seen this for copywriters, artists and translators.\\nThere are a great deal of untold stories here. I‚Äôm hoping 2024 sees significant amounts of dedicated journalism on this topic.\\nMy blog in 2023\\nHere‚Äôs a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '749d7140-190b-438b-9a26-24656b0f29ac'}, page_content='The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\\nI‚Äôve written a lot about this stuff!\\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\\n\\n\\n\\nArticle\\nVisitors\\nPageviews\\n\\n\\n\\n\\nBing: ‚ÄúI will not harm you unless you harm me first‚Äù\\n1.1M\\n1.3M\\n\\n\\nLeaked Google document: ‚ÄúWe Have No Moat, And Neither Does OpenAI‚Äù\\n132k\\n162k\\n\\n\\nLarge language models are having their Stable Diffusion moment\\n121k\\n150k\\n\\n\\nPrompt injection: What‚Äôs the worst that can happen?\\n79.8k\\n95.9k'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'dcca5043-2fa2-4d06-a054-5744cbdecb0d'}, page_content='Embeddings: What they are and why they matter\\n61.7k\\n79.3k\\n\\n\\nCatching up on the weird world of LLMs\\n61.6k\\n85.9k\\n\\n\\nllamafile is the new best way to run an LLM on your own computer\\n52k\\n66k\\n\\n\\nPrompt injection explained, with video, slides, and a transcript\\n51k\\n61.9k\\n\\n\\nAI-enhanced development makes me more ambitious with my projects\\n49.6k\\n60.1k\\n\\n\\nUnderstanding GPT tokenizers\\n49.5k\\n61.1k\\n\\n\\nExploring GPTs: ChatGPT in a trench coat?\\n46.4k\\n58.5k\\n\\n\\nCould you train a ChatGPT-beating model for $85,000 and run it in a browser?\\n40.5k\\n49.2k\\n\\n\\nHow to implement Q&A against your documentation with GPT3, embeddings and Datasette\\n37.3k\\n44.9k\\n\\n\\nLawyer cites fake cases invented by ChatGPT, judge is not amused\\n37.1k\\n47.4k'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '0eb549a0-4912-4ce7-b931-7e623445d762'}, page_content='Now add a walrus: Prompt engineering in DALL-E 3\\n32.8k\\n41.2k\\n\\n\\nWeb LLM runs the vicuna-7b Large Language Model entirely in your browser, and it‚Äôs very impressive\\n32.5k\\n38.2k\\n\\n\\nChatGPT can‚Äôt access the internet, even though it really looks like it can\\n30.5k\\n34.2k\\n\\n\\nStanford Alpaca, and the acceleration of on-device large language model development\\n29.7k\\n35.7k\\n\\n\\nRun Llama 2 on your own Mac using LLM and Homebrew\\n27.9k\\n33.6k\\n\\n\\nMidjourney 5.1\\n26.7k\\n33.4k\\n\\n\\nThink of language models like ChatGPT as a ‚Äúcalculator for words‚Äù\\n25k\\n31.8k\\n\\n\\nMulti-modal prompt injection image attacks against GPT-4V\\n23.7k\\n27.4k'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '9b86a82d-9fe5-4fe6-be9c-e8ee62346338'}, page_content='I also gave a bunch of talks and podcast appearances. I‚Äôve started habitually turning my talks into annotated presentations‚Äîhere are my best from 2023:\\n\\nPrompt injection explained, with video, slides, and a transcript\\nCatching up on the weird world of LLMs\\nMaking Large Language Models work for you\\nOpen questions for AI engineering\\nEmbeddings: What they are and why they matter\\nFinancial sustainability for open source projects at GitHub Universe\\n\\nAnd in podcasts:\\n\\n\\nWhat AI can do for you on the Theory of Change\\n\\nWorking in public on Path to Citus Con\\n\\nLLMs break the internet on the Changelog\\n\\nTalking Large Language Models on Rooftop Ruby\\n\\nThoughts on the OpenAI board situation on Newsroom Robots'),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': 'd23e88b0-28f1-4ba1-83f5-4f974244180d'}, page_content=\"Industry‚Äôs Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\\n\\n\\nPosted 31st December 2023 at 11:59 pm ¬∑ Follow me on Mastodon or Twitter or subscribe to my newsletter\\n\\n\\nMore recent articles\\n\\nRun LLMs on macOS using llm-mlx and Apple's MLX framework - 15th February 2025\\nURL-addressable Pyodide Python environments - 13th February 2025\\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\\n\\n\\n \\n\\n\\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\\n\\nPart of series LLMs annual review\\n\\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. \\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\"),\n",
              " Document(metadata={'source': 'data/2023_llms.html', 'title': 'Stuff we figured out about AI in 2023', 'id': '19ea4809-f47f-4cf5-ba34-acf3ddf25ab8'}, page_content='blogging\\n            68\\n\\n\\n            ai\\n            1098\\n\\n\\n            generative-ai\\n            942\\n\\n\\n            llms\\n            930\\n\\nNext: Tom Scott, and the formidable power of escalating streaks\\nPrevious: Last weeknotes of 2023\\n\\n\\n \\n \\n\\n\\nColophon\\n¬©\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n2025'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '606fc32f-2a04-4d19-a153-e34fd7f1c415'}, page_content='Things we learned about LLMs in 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSimon Willison‚Äôs Weblog\\nSubscribe\\n\\n\\n\\n\\n\\n\\nThings we learned about LLMs in 2024\\n31st December 2024\\nA lot has happened in the world of Large Language Models over the course of 2024. Here‚Äôs a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\\nThis is a sequel to my review of 2023.\\nIn this article:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'f7cd97cf-97ff-45ad-8473-8ec25fdf2041'}, page_content='The GPT-4 barrier was comprehensively broken\\nSome of those GPT-4 models run on my laptop\\nLLM prices crashed, thanks to competition and increased efficiency\\nMultimodal vision is common, audio and video are starting to emerge\\nVoice and live camera mode are science fiction come to life\\nPrompt driven app generation is a commodity already\\nUniversal access to the best models lasted for just a few short months\\n‚ÄúAgents‚Äù still haven‚Äôt really happened yet\\nEvals really matter\\nApple Intelligence is bad, Apple‚Äôs MLX library is excellent\\nThe rise of inference-scaling ‚Äúreasoning‚Äù models\\nWas the best currently available LLM trained in China for less than $6m?\\nThe environmental impact got better\\nThe environmental impact got much, much worse'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '4510521b-112d-4e24-915e-fdadb4718d80'}, page_content='The year of slop\\nSynthetic training data works great\\nLLMs somehow got even harder to use\\nKnowledge is incredibly unevenly distributed\\nLLMs need better criticism\\nEverything tagged ‚Äúllms‚Äù on my blog in 2024'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '434eca21-6360-44ee-910f-46d0ceee7493'}, page_content='The GPT-4 barrier was comprehensively broken\\nIn my December 2023 review I wrote about how We don‚Äôt yet know how to build GPT-4‚ÄîOpenAI‚Äôs best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn‚Äôt?\\nI‚Äôm relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)‚Äî70 models in total.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '2a89264c-2c47-4c04-8b7a-32b9c4b2d81e'}, page_content='The earliest of those was Google‚Äôs Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field‚Äîmost notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'cfa6aa53-7fce-45d5-a2e0-abc812c628cd'}, page_content='Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google‚Äôs Gemini series accepts up to 2 million.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '2971936c-c007-4d32-939c-c01766b43d2d'}, page_content='Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '97c8fedf-4bee-4d79-930e-8dd4953a27f8'}, page_content='Getting back to models that beat GPT-4: Anthropic‚Äôs Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet‚Äîa model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'c64e4e53-6e63-4127-b616-9db4fc0262ab'}, page_content='Then there‚Äôs the rest. If you browse the Chatbot Arena leaderboard today‚Äîstill the most useful single place to get a vibes-based evaluation of models‚Äîyou‚Äôll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it‚Äôs an achievement that isn‚Äôt even particularly notable, though I personally still celebrate any time a new organization joins that list.\\nSome of those GPT-4 models run on my laptop'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'af5628c8-059e-4175-969e-6792173eaeff'}, page_content='My personal laptop is a 64GB M2 MacBook Pro from 2023. It‚Äôs a powerful machine, but it‚Äôs also nearly two years old now‚Äîand crucially it‚Äôs the same laptop I‚Äôve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'b4d4d3a9-e5c2-4201-b595-5eea6144f87a'}, page_content='Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November‚Äîan Apache 2.0 licensed model!\\n\\nI can now run a GPT-4 class model on my laptop talks about running Meta‚Äôs Llama 3.3 70B (released in December)'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '3ed817a9-c8ff-469c-8a4f-292857e1e83b'}, page_content='This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\\nThese models take up enough of my 64GB of RAM that I don‚Äôt run them often‚Äîthey don‚Äôt leave much room for anything else.\\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we‚Äôve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there‚Äôs still more to come.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'cdc40afe-1282-48f3-850e-0da73c441afc'}, page_content='Meta‚Äôs Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it‚Äôs a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for ‚Äúa plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist‚Äù. Here‚Äôs what I got, at a respectable 20 tokens per second:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'ec400857-025a-4ba8-871e-381e9212115e'}, page_content='Here‚Äôs the rest of the transcript. It‚Äôs bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\\nLLM prices crashed, thanks to competition and increased efficiency\\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\\nIn December 2023 (here‚Äôs the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\\nToday $30/mTok gets you OpenAI‚Äôs most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok‚Äînearly 7x cheaper than GPT-3.5 and massively more capable.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '6a650465-3a6c-4d41-b778-d7c9310478a1'}, page_content='Other model providers charge even less. Anthropic‚Äôs Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google‚Äôs Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok‚Äîthat‚Äôs 27x cheaper than GPT-3.5 Turbo last year.\\nI‚Äôve been tracking these pricing changes under my llm-pricing tag.\\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '9d4aec4a-ffec-4058-83ed-af66f301c0ec'}, page_content='There‚Äôs still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\\nHere‚Äôs a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google‚Äôs Gemini 1.5 Flash 8B (released in October), their cheapest model?\\nEach photo would need 260 input tokens and around 100 output tokens.\\n260 * 68,000 = 17,680,000 input tokens\\n17,680,000 * $0.0375/million = $0.66\\n100 * 68,000 = 6,800,000 output tokens\\n6,800,000 * $0.15/million = $1.02'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '2bcb719d-09f7-4924-8eed-4a0247b6be10'}, page_content='That‚Äôs a total cost of $1.68 to process 68,000 images. That‚Äôs so absurdly cheap I had to run the numbers three times to confirm I got it right.\\nHow good are those descriptions? Here‚Äôs what I got from this command:\\nllm -m gemini-1.5-flash-8b-latest describe -a IMG_1825.jpeg'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '559b6289-102b-44a0-8607-300d42ed1ed0'}, page_content='Against this photo of butterflies at the California Academy of Sciences:\\n\\n\\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'f5fad84a-a129-44ec-9d59-321447152c61'}, page_content='260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that‚Äôs less than a 400th of a cent).\\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that‚Äôs what we‚Äôre getting.\\nMultimodal vision is common, audio and video are starting to emerge\\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI‚Äôs DevDay in November 2023. Google‚Äôs multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '8ca0819e-63b7-4369-b38d-e83f83177310'}, page_content='In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral‚Äôs Pixtral 12B and Meta‚Äôs Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '41c4a5f1-467c-489c-95c0-2be35c45149a'}, page_content='I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\\nVoice and live camera mode are science fiction come to life\\nThe audio and live video modes that have started to emerge deserve a special mention.\\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'c7fb9f21-471d-4aad-8ca0-3bc21012cbe6'}, page_content='The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for ‚Äúomni‚Äù) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in ‚Äú4o‚Äù mode is not running the new features yet.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '25b6d00e-aafb-4dcb-9451-11db2f39d42b'}, page_content='When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I‚Äôve been using it extensively on walks with my dog and it‚Äôs amazing how much the improvement in intonation elevates the material. I‚Äôve also had a lot of fun experimenting with the OpenAI audio APIs.\\nEven more fun: Advanced Voice mode can do accents! Here‚Äôs what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'a0ecf954-a488-47a6-91a9-84ff266fe5a5'}, page_content='Your browser does not support the audio element.\\n\\nOpenAI aren‚Äôt the only group with a multi-modal audio model. Google‚Äôs Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that‚Äôs meant to roll out in Q1 of 2025.\\nGoogle‚Äôs NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two ‚Äúpodcast hosts‚Äù about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\\n\\n\\nYour browser does not support the audio element.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '07f5e86b-4853-4938-b392-dce312a605e9'}, page_content='The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '05309d3f-36e7-4c92-a485-b95f566066ed'}, page_content='These abilities are just a few weeks old at this point, and I don‚Äôt think their impact has been fully felt yet. If you haven‚Äôt tried them out yet you really should.\\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\\nPrompt driven app generation is a commodity already\\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'a30144db-a127-437b-837e-6e93c2a38cf4'}, page_content='We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)‚Äîoften in a single prompt.\\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\\nHere‚Äôs my Extract URLs app, entirely generated by Claude:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'b984f60e-78c1-4bcd-8e02-c680abb1ef01'}, page_content='I‚Äôve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this‚ÄîGitHub Spark‚Äîin October. Mistral Chat added it as a feature called Canvas in November.\\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'c3356946-d198-45d1-9dca-5a3c7b9e472a'}, page_content='Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\\nI‚Äôve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'b7fe1ced-7414-4f8f-9458-104f553ce57d'}, page_content='This prompt-driven custom interface feature is so powerful and easy to build (once you‚Äôve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\\nUniversal access to the best models lasted for just a few short months\\nFor a few short months this year all three of the best available models‚ÄîGPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro‚Äîwere freely available to most of the world.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'ceba8b42-651b-4865-acb6-a2f9c2f2a795'}, page_content='OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\\nThat era appears to have ended, likely permanently, with OpenAI‚Äôs launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don‚Äôt think those days of free access to the best available models are likely to return.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '7c18ee8a-bed3-4efc-9a88-15c2f3f7a934'}, page_content='‚ÄúAgents‚Äù still haven‚Äôt really happened yet\\nI find the term ‚Äúagents‚Äù extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\\nIf you tell me that you are building ‚Äúagents‚Äù, you‚Äôve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'ea9f2842-6e5e-4188-a070-4a6a016f6687'}, page_content='The two main categories I see are people who think AI agents are obviously things that go and act on your behalf‚Äîthe travel agent model‚Äîand people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term ‚Äúautonomy‚Äù is often thrown into the mix too, again without including a clear definition.\\n(I also collected 211 definitions on Twitter a few months ago‚Äîhere they are in Datasette Lite‚Äîand had gemini-exp-1206 attempt to summarize them.)\\nWhatever the term may mean, agents still have that feeling of perpetually ‚Äúcoming soon‚Äù.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '4efd0e35-f4d3-4a1d-b8ff-a03656cbdf44'}, page_content='Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can‚Äôt distinguish truth from fiction?\\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie ‚ÄúEncanto 2‚Äù. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '8a3eeda0-a956-4512-bd51-10d2aa62b02e'}, page_content='Prompt injection is a natural consequence of this gulibility. I‚Äôve seen precious little progress on tackling that problem in 2024, and we‚Äôve been talking about it since September 2022.\\nI‚Äôm beginning to see the most popular idea of ‚Äúagents‚Äù as dependent on AGI itself. A model that‚Äôs robust against gulliblity is a very tall order indeed.\\nEvals really matter\\nAnthropic‚Äôs Amanda Askell (responsible for much of the work behind Claude‚Äôs Character):'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'fb589668-268e-43fa-bd9b-c92c8c4feaf8'}, page_content='The boring yet crucial secret behind good system prompts is test-driven development. You don‚Äôt write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\\n\\nIt‚Äôs become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that‚Äôs most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\\nVercel‚Äôs Malte Ubl:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'adb1ea33-ce4c-4506-8d0a-c547b1c07eb8'}, page_content='When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '0697d34f-3d83-4949-add4-dc0d0209132f'}, page_content='I‚Äôm still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them‚ÄîI‚Äôm tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\\nApple Intelligence is bad, Apple‚Äôs MLX library is excellent\\nAs a Mac user I‚Äôve been feeling a lot better about my choice of platform this year.\\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'cfc1e5f3-d466-46c9-9f49-1869bb071838'}, page_content='On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA‚Äôs CUDA over other platforms.\\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple‚Äôs MLX library, ‚Äúan array framework for Apple Silicon‚Äù. It‚Äôs fantastic.\\nApple‚Äôs mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '80bd81b2-e260-4124-9833-fc911c543dfe'}, page_content='Prince Canuma‚Äôs excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen‚Äôs QvQ.\\nWhile MLX is a game changer, Apple‚Äôs own ‚ÄúApple Intelligence‚Äù features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'eb95bb28-4434-476f-8d4e-556c887f4fbf'}, page_content='Now that those features are rolling out they‚Äôre pretty weak. As an LLM power-user I know what these models are capable of, and Apple‚Äôs LLM features offer a pale imitation of what a frontier LLM can do. Instead we‚Äôre getting notification summaries that misrepresent news headlines and writing assistant tools that I‚Äôve not found useful at all. Genmoji are kind of fun though.\\nThe rise of inference-scaling ‚Äúreasoning‚Äù models\\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI‚Äôs o1 models‚Äîinitially released as o1-preview and o1-mini on September 12th.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '20054912-919c-4f1a-a3e5-70d062cd5c1c'}, page_content='One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\\nThis is that trick where, if you get a model to talk out loud about a problem it‚Äôs solving, you often get a result which the model would not have achieved otherwise.\\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend ‚Äúreasoning tokens‚Äù thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'e8fa57ac-7d3b-4887-8743-79db65c00325'}, page_content='The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\\nThe sequel to o1, o3 (they skipped ‚Äúo2‚Äù for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure‚ÄîI certainly don‚Äôt!‚Äîbut it appears to be a genuine next step in LLM architecture for taking on much harder problems.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '6ea901f2-7ff3-41ce-ad71-7592bdf8cb3c'}, page_content='OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\\nAlibaba‚Äôs Qwen team released their QwQ model on November 28th‚Äîunder an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '5f4e632c-b4b9-4d53-9fc5-4c06b2c57f3c'}, page_content='Nothing yet from Anthropic or Meta but I would be very surprised if they don‚Äôt have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\\nWas the best currently available LLM trained in China for less than $6m?\\nNot quite, but almost! It does make for a great attention-grabbing headline.\\nThe big news to end the year was the release of DeepSeek v3‚Äîdropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'f88aad2d-6a4e-496b-9bec-d61fcd7ccd3d'}, page_content='DeepSeek v3 is a huge 685B parameter model‚Äîone of the largest openly licensed models currently available, significantly bigger than the largest of Meta‚Äôs Llama series, Llama 3.1 405B.\\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours‚Äî11x that used by DeepSeek v3, for a model that benchmarks slightly worse.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'cb2e06a1-9d5e-4d5f-91f6-f1c28fad1762'}, page_content='Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\\nThe environmental impact got better\\nA welcome result of the increased efficiency of the models‚Äîboth the hosted ones and the ones I can run locally‚Äîis that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '581406c3-4d5e-4c54-9355-277b638a831f'}, page_content='I think this means that, as individual users, we don‚Äôt need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That‚Äôs certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '67766790-b7f8-40b4-869c-7a59a0d9513f'}, page_content='The environmental impact got much, much worse\\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There‚Äôs even talk of spinning up new nuclear power stations, but those can take decades.\\nIs this infrastructure necessary? DeepSeek v3‚Äôs $6m training cost and the continued crash in LLM prices might hint that it‚Äôs not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years‚Äô time?'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'a6ec58d4-5c48-4dfb-9afd-5d537220f3e2'}, page_content='An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary‚Äîsometimes multiple lines from different companies serving the exact same routes!\\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK‚Äôs Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\\nThe year of slop'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '0c9741d3-e236-49ed-8df3-0ca97ef358f1'}, page_content='The year of slop\\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '830820fc-8a0d-4417-9f2b-9993e752a1c2'}, page_content='Watching in real time as ‚Äúslop‚Äù becomes a term of art. the way that ‚Äúspam‚Äù became the term for unwanted emails, ‚Äúslop‚Äù is going in the dictionary as the term for unwanted AI generated content\\n\\nI expanded that definition a tiny bit to this:\\n\\nSlop describes AI-generated content that is both unrequested and unreviewed.\\n\\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here‚Äôs what I said in the NY TImes:\\n\\nSociety needs concise ways to talk about modern A.I. ‚Äî both the positives and the negatives. ‚ÄòIgnore that email, it‚Äôs spam,‚Äô and ‚ÄòIgnore that article, it‚Äôs slop,‚Äô are both useful lessons.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '74e41a7f-a554-4990-ae7c-eab3e57c482e'}, page_content='I love the term ‚Äúslop‚Äù because it so succinctly captures one of the ways we should not be using generative AI!\\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\\nSynthetic training data works great\\nAn idea that surprisingly seems to have stuck in the public consciousness is that of ‚Äúmodel collapse‚Äù. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'd72c0aa3-5ce9-4435-816c-45ad50193424'}, page_content='The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\\nThat‚Äôs clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content‚Äîdeliberately creating artificial data to help steer their models in the right way.\\nOne of the best descriptions I‚Äôve seen of this comes from the Phi-4 technical report, which included this:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'a41be909-0807-47e9-9f14-32ec8e0e5e05'}, page_content='Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '4fa4400d-9621-4029-bb59-4b7a9ba43739'}, page_content='Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'c9dcc782-7c97-498c-96a7-35db3b3ceec6'}, page_content='Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives‚Äîa trick used by an increasing number of labs. DeepSeek v3 used ‚Äúreasoning‚Äù data created by DeepSeek-R1. Meta‚Äôs Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\\nLLMs somehow got even harder to use'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '01db3353-3aae-462b-a30d-c355612bda27'}, page_content='A drum I‚Äôve been banging for a while is that LLMs are power-user tools‚Äîthey‚Äôre chainsaws disguised as kitchen knives. They look deceptively simple to use‚Äîhow hard can it be to type messages to a chatbot?‚Äîbut in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\\nIf anything, this problem got worse in 2024.\\nWe‚Äôve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it‚Äôs accurately reflected in the undocumented and secret training set.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'eb05ac2b-3b54-484f-9e9d-bf5fa980b87a'}, page_content='The number of available systems has exploded. Different systems have different tools they can apply to your problems‚Äîlike Python and JavaScript and web search and image generation and maybe even database lookups... so you‚Äôd better understand what those tools are, what they can do and how to tell if the LLM used them or not.\\nDid you know ChatGPT has two entirely different ways of running Python now?\\nWant to build a Claude Artifact that talks to an external API? You‚Äôd better understand CSP and CORS HTTP headers first.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '0f97d47f-dbe8-45a8-b6fe-ca1d80f33e4d'}, page_content='The models may have got more capable, but most of the limitations remained the same. OpenAI‚Äôs o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it‚Äôs running in. o1 can‚Äôt run web searches or use Code Interpreter, but GPT-4o can‚Äîboth in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '498b2af3-9ed5-456f-a897-876d49e928cd'}, page_content='Meanwhile, it‚Äôs increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I‚Äôve seen so many examples of people trying to win an argument with a screenshot from ChatGPT‚Äîan inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'b21081a2-f0ee-43ef-afb2-93df704e02cb'}, page_content='There‚Äôs a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can‚Äôt see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\\nKnowledge is incredibly unevenly distributed\\nMost people have heard of ChatGPT by now. How many have heard of Claude?'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '3f516b02-c270-440c-889e-d755a57b3612'}, page_content='The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\\nThe pace of change doesn‚Äôt help either. In just the past month we‚Äôve seen general availability of live interfaces where you can point your phone‚Äôs camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven‚Äôt even tried that yet.\\nGiven the ongoing (and potential) impact on society that this technology has, I don‚Äôt think the size of this gap is healthy. I‚Äôd like to see a lot more effort put into improving this.\\nLLMs need better criticism'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '82ecb6f2-efc0-4661-a417-6208a207da63'}, page_content='A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that ‚ÄúLLMs are useful‚Äù can be enough to kick off a huge fight.\\nI get it. There are plenty of reasons to dislike this technology‚Äîthe environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people‚Äôs jobs.\\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '565ad4c9-5058-4707-ac06-4e745c7b9f65'}, page_content='I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\\n(If you still don‚Äôt think there are any good applications at all I‚Äôm not sure why you made it to this point in the article!)'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '8a5d1b36-a6c0-4a9a-95d2-e833d1f21c44'}, page_content='I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\\nThose of us who understand this stuff have a duty to help everyone else figure it out.\\nEverything tagged ‚Äúllms‚Äù on my blog in 2024\\nBecause I undoubtedly missed a whole bunch of things, here‚Äôs every long-form post I wrote in 2024 that I tagged with llms:'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '7863fe22-8bcf-47f0-8478-5d925f422c1f'}, page_content='January\\n\\n7th: It‚Äôs OK to call it Artificial Intelligence\\n\\n9th: What I should have said about the term Artificial Intelligence\\n\\n17th: Talking about Open Source LLMs on Oxide and Friends\\n\\n26th: LLM 0.13: The annotated release notes\\n\\n\\n\\nFebruary\\n\\n21st: The killer app of Gemini Pro 1.5 is video\\n\\n\\n\\nMarch\\n\\n5th: Prompt injection and jailbreaking are not the same thing\\n\\n8th: The GPT-4 barrier has finally been broken\\n\\n22nd: Claude and ChatGPT for ad-hoc sidequests\\n\\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\\n\\n26th: llm cmd undo last git commit‚Äîa new plugin for LLM\\n\\n\\n\\nApril\\n\\n8th: Building files-to-prompt entirely using Claude 3 Opus\\n\\n10th: Three major LLM releases in 24 hours (plus weeknotes)'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '712b2692-29c8-4695-9dcc-f5e0f4d6a940'}, page_content='17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\\n\\n22nd: Options for accessing Llama 3 from the terminal using LLM\\n\\n\\n\\nMay\\n\\n8th: Slop is the new name for unwanted AI-generated content\\n\\n15th: ChatGPT in ‚Äú4o‚Äù mode is not running the new features yet\\n\\n29th: Training is not the same as chatting: ChatGPT and other LLMs don‚Äôt remember everything you say\\n\\n\\n\\nJune\\n\\n6th: Accidental prompt injection against RAG applications\\n\\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\\n\\n17th: Language models on the command-line\\n\\n21st: Building search-based RAG using Claude, Datasette and Val Town\\n\\n27th: Open challenges for AI engineering\\n\\n\\n\\nJuly\\n\\n14th: Imitation Intelligence, my keynote for PyCon US 2024'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'e94c22e5-40dc-438a-a598-411d31277b33'}, page_content='19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\\n\\n\\n\\nAugust\\n\\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\\n\\n8th: django-http-debug, a new Django app mostly written by Claude\\n\\n23rd: Claude‚Äôs API now supports CORS requests, enabling client-side applications\\n\\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\\n\\n\\n\\nSeptember\\n\\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\\n\\n10th: Notes from my appearance on the Software Misadventures Podcast\\n\\n12th: Notes on OpenAI‚Äôs new o1 chain-of-thought models\\n\\n20th: Notes on using LLMs for code'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'a9ff7e82-075c-4bbe-b0f9-43dbbb61c853'}, page_content='29th: NotebookLM‚Äôs automatically generated podcasts are surprisingly effective\\n\\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\\n\\n\\n\\nOctober\\n\\n1st: OpenAI DevDay 2024 live blog\\n\\n2nd: OpenAI DevDay: Let‚Äôs build developer tools, not digital God\\n\\n15th: ChatGPT will happily write you a thinly disguised horoscope\\n\\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\\n\\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\\n\\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\\n\\n21st: Everything I built with Claude Artifacts this week\\n\\n22nd: Initial explorations of Anthropic‚Äôs new Computer Use capability'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'dd6f5c04-af7c-4cc7-9635-f0ff67c78e4f'}, page_content='24th: Notes on the new Claude analysis JavaScript code execution tool\\n\\n27th: Run a prompt to generate and execute jq programs using llm-jq\\n\\n29th: You can now run prompts against images, audio and video in your terminal using LLM\\n\\n30th: WÃ∂eÃ∂eÃ∂kÃ∂nÃ∂oÃ∂tÃ∂eÃ∂sÃ∂  Monthnotes for October\\n\\n\\n\\nNovember\\n\\n4th: Claude 3.5 Haiku\\n\\n7th: Project: VERDAD‚Äîtracking misinformation in radio broadcasts using Gemini 1.5\\n\\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\\n\\n19th: Notes from Bing Chat‚ÄîOur First Encounter With Manipulative AI\\n\\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\\n\\n\\n\\nDecember\\n\\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\\n\\n7th: Prompts.js'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': 'd1bd8b61-88f1-435b-9a7a-2f3ca71ec069'}, page_content='7th: Prompts.js\\n\\n9th: I can now run a GPT-4 class model on my laptop\\n\\n10th: ChatGPT Canvas can make API requests now, but it‚Äôs complicated\\n\\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\\n\\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\\n\\n19th: Gemini 2.0 Flash ‚ÄúThinking mode‚Äù\\n\\n20th: December in LLMs has been a lot\\n\\n20th: Live blog: the 12th day of OpenAI‚Äî‚ÄúEarly evals for OpenAI o3‚Äù\\n\\n24th: Trying out QvQ‚ÄîQwen‚Äôs new visual reasoning model\\n\\n31st: Things we learned about LLMs in 2024\\n\\n\\n\\n\\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)'),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '4f33d6f2-a9cf-4b03-abbc-e50e000e2329'}, page_content=\"Posted 31st December 2024 at 6:07 pm ¬∑ Follow me on Mastodon or Twitter or subscribe to my newsletter\\n\\n\\nMore recent articles\\n\\nRun LLMs on macOS using llm-mlx and Apple's MLX framework - 15th February 2025\\nURL-addressable Pyodide Python environments - 13th February 2025\\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\\n\\n\\n \\n\\n\\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\\n\\nPart of series LLMs annual review\\n\\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. \\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. \\n\\n\\n\\n            google\\n            347\\n\\n\\n            ai\\n            1098\\n\\n\\n            openai\\n            255\"),\n",
              " Document(metadata={'source': 'data/2024_llms.html', 'title': 'Things we learned about LLMs in 2024', 'id': '3ee032da-0ca8-477f-abc9-f3797ace452e'}, page_content=\"generative-ai\\n            942\\n\\n\\n            llms\\n            930\\n\\n\\n            anthropic\\n            114\\n\\n\\n            gemini\\n            57\\n\\n\\n            meta\\n            26\\n\\n\\n            inference-scaling\\n            28\\n\\n\\n            long-context\\n            10\\n\\nNext: Ending a year long posting streak\\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\\n\\n\\n \\n \\n\\n\\nColophon\\n¬©\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n2025\")]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnL4oNg341U"
      },
      "source": [
        "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MTS4GTSEcnG4"
      },
      "outputs": [],
      "source": [
        "training_split_documents = training_documents[:len(training_documents) - 24]\n",
        "val_split_documents = training_documents[len(training_documents) - 24:102-12]\n",
        "test_split_documents = training_documents[102-12:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlvKbONDWvQ"
      },
      "source": [
        "## Task 3: Constructing a Fine-tuning Dataset\n",
        "\n",
        "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` (released [today](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)).\n",
        "\n",
        "The basic idea here is straightforward enough:\n",
        "\n",
        "1. We look at a document\n",
        "2. We generate questions that could be answered by that node\n",
        "\n",
        "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_EWfmIscMrvg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "qa_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hLnsSB6Y-S"
      },
      "source": [
        "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "diEWcw00NMSj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = \"\"\"\\\n",
        "Given the following context, you must generate questions based on only the provided context.\n",
        "\n",
        "You are to generate {n_questions} questions which should be provided in the following format:\n",
        "\n",
        "1. QUESTION #1\n",
        "2. QUESTION #2\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'n_questions'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'n_questions'], input_types={}, partial_variables={}, template='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate {n_questions} questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n{context}\\n'), additional_kwargs={})])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_prompt_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u87Izpgm6_fk"
      },
      "source": [
        "We'll create a simple chain to query the LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ggl9SSjiNbpG"
      },
      "outputs": [],
      "source": [
        "question_generation_chain = qa_prompt_template | qa_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duvHirh7DQv"
      },
      "source": [
        "There's a lot going on in this function - let's take a deeper look:\n",
        "\n",
        "1. First, we provide a list of documents and a number of questions\n",
        "2. We, for each document in our list, generate `n_questions` of questions.\n",
        "3. We then associate those questions and contexts via a `UUID`.\n",
        "\n",
        "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lm2JvgC9X37"
      },
      "source": [
        "##### üèóÔ∏è Activity #1:\n",
        "\n",
        "We have:\n",
        "\n",
        "- Lists of `Documents` with the `metadata` field `id`.\n",
        "\n",
        "We need:\n",
        "\n",
        "- An object with key `id`, which have values `str` questions.\n",
        "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
        "\n",
        "An Example:\n",
        "\n",
        "question_object:\n",
        "```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
        "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
        "}\n",
        " ```\n",
        "\n",
        " context_object:\n",
        " ```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "}\n",
        " ```\n",
        "\n",
        " As you can see, a piece of context can be associated with more than 1 question.\n",
        "\n",
        " The task is to write the Python function(s) to accomplish this task.\n",
        "\n",
        " Your function signature is provided below, along with the desired return values.\n",
        "\n",
        " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U4yi4NfTCnLc"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import re\n",
        "\n",
        "async def create_questions(documents, n_questions):\n",
        "    questions = {}\n",
        "    relevant_docs = {}\n",
        "\n",
        "    for document in documents:\n",
        "        # Generate questions using the chat model\n",
        "        question_response = await qa_prompt_template.ainvoke({\"context\": document.page_content, \"n_questions\": n_questions})\n",
        "        \n",
        "        # Debug: Print the type and structure of the response\n",
        "        print(f\"Type of response: {type(question_response)}\")\n",
        "        print(f\"Dir of response: {dir(question_response)}\")\n",
        "        print(f\"Response: {question_response}\")\n",
        "        \n",
        "        try:\n",
        "            # Try different ways to get the content\n",
        "            if hasattr(question_response, 'text'):\n",
        "                question_text = question_response.text\n",
        "            elif hasattr(question_response, 'message'):\n",
        "                question_text = question_response.message.content\n",
        "            else:\n",
        "                # If all else fails, try string conversion\n",
        "                question_text = str(question_response)\n",
        "            \n",
        "            # Parse the numbered questions from the response\n",
        "            parsed_questions = re.findall(r'\\d+\\.\\s+(.*?)(?=\\d+\\.|$)', question_text, re.DOTALL)\n",
        "            \n",
        "            # For each parsed question, create a unique ID and store the question\n",
        "            for question in parsed_questions:\n",
        "                question = question.strip()  # Clean up any extra whitespace\n",
        "                if question:  # Only process non-empty questions\n",
        "                    question_id = str(uuid.uuid4())\n",
        "                    questions[question_id] = question\n",
        "                    relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document: {e}\")\n",
        "            raise\n",
        "\n",
        "    return questions, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0eWOUo4QGL"
      },
      "source": [
        "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Dq6KRqEs0F",
        "outputId": "60dcd580-b6e8-4e3b-d605-05b492ca5c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nStuff we figured out about AI in 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSimon Willison‚Äôs Weblog\\nSubscribe\\n\\n\\n\\n\\n\\n\\nStuff we figured out about AI in 2023\\n31st December 2023\\n2023 was the breakthrough year for Large Language Models (LLMs). I think it‚Äôs OK to call these AI‚Äîthey‚Äôre the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\\nHere‚Äôs my attempt to round up the highlights in one place!\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nLarge Language Models\\nThey‚Äôre actually quite easy to build\\nYou can run LLMs on your own devices\\nHobbyists can build their own fine-tuned models\\nWe don‚Äôt yet know how to build GPT-4\\nVibes Based Development\\nLLMs are really smart, and also really, really dumb\\nGullibility is the biggest unsolved problem\\nCode may be the best application\\nThe ethics of this space remain diabolically complex\\nMy blog in 2023\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nHere‚Äôs the sequel to this post: Things we learned about LLMs in 2024.\\nLarge Language Models\\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nSo far, I think they‚Äôre a net positive. I‚Äôve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\\nThey‚Äôre actually quite easy to build\\nThe most surprising thing we‚Äôve learned about LLMs this year is that they‚Äôre actually quite easy to build.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nA year ago, the only organization that had released a generally useful LLM was OpenAI. We‚Äôve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\\nThe training cost (hardware and electricity) is still significant‚Äîinitially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft‚Äôs Phi-2 claims to have used ‚Äú14 days on 96 A100 GPUs‚Äù, which works out at around $35,000 using current Lambda pricing.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nSo training an LLM still isn‚Äôt something a hobbyist can afford, but it‚Äôs no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge‚Äînot trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia‚Äôs Suspension bridges by country category lists 44 countries).\\nYou can run LLMs on your own devices\\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2‚Äîan improved version which, crucially, included permission for commercial use.\\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\\nHobbyists can build their own fine-tuned models\\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThere‚Äôs now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can‚Äôt even attempt to count them, and any count would be out-of-date within a few hours.\\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it‚Äôs whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\\nThis is a huge advantage for open over closed models: the closed, hosted models don‚Äôt have thousands of researchers and hobbyists around the world collaborating and competing to improve them.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nWe don‚Äôt yet know how to build GPT-4\\nFrustratingly, despite the enormous leaps ahead we‚Äôve had this year, we are yet to see an alternative model that‚Äôs better than GPT-4.\\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\\nThis may well change in the next few weeks: Google‚Äôs Gemini Ultra has big claims, but isn‚Äôt yet available for us to try out.\\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they‚Äôve released two significant improvements since then.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nStill, I‚Äôm surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven‚Äôt shared yet.\\nVibes Based Development\\nAs a computer scientist and software engineer, LLMs are infuriating.\\nEven the openly licensed ones are still the world‚Äôs most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\\nI‚Äôm used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\\nThe worst part is the challenge of evaluating them.\\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually ‚Äúfeels‚Äù right when you try it for a given task.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI find I have to work with an LLM for a few weeks in order to get a good intuition for it‚Äôs strengths and weaknesses. This greatly limits how many I can evaluate myself!\\nThe most frustrating thing for me is at the level of individual prompting.\\nSometimes I‚Äôll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don‚Äôt have a good methodology for figuring that out.\\nWe‚Äôre left with what‚Äôs effectively Vibes Based Development. It‚Äôs vibes all the way down.\\nI‚Äôd love to see us move beyond vibes in 2024!\\nLLMs are really smart, and also really, really dumb\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nOn the one hand, we keep on finding new things that LLMs can do that we didn‚Äôt expect‚Äîand that the people who trained the models didn‚Äôt expect either. That‚Äôs usually really fun!\\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\\nThe honest answer is ‚Äúmaybe‚Äù! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nSometimes it omits sections of code and leaves you to fill them in, but if you tell it you can‚Äôt type because you don‚Äôt have any fingers it produces the full code for you instead.\\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It‚Äôs all so dumb, but it works!\\nGullibility is the biggest unsolved problem\\nI coined the term prompt injection in September last year.\\n15 months later, I regret to say that we‚Äôre still no closer to a robust, dependable solution to this problem.\\nI‚Äôve written a ton about this already.\\nBeyond that specific class of security vulnerabilities, I‚Äôve started seeing this as a wider problem of gullibility.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nLanguage Models are gullible. They ‚Äúbelieve‚Äù what we tell them‚Äîwhat‚Äôs in their training data, then what‚Äôs in the fine-tuning data, then what‚Äôs in the prompt.\\nIn order to be useful tools for us, we need them to believe what we feed them!\\nBut it turns out a lot of the things we want to build need them not to be gullible.\\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nA lot of people are excited about AI agents‚Äîan infuriatingly vague term that seems to be converging on ‚ÄúAI systems that can go away and act on your behalf‚Äù. We‚Äôve been talking about them all year, but I‚Äôve seen few if any examples of them running in production, despite lots of exciting prototypes.\\nI think this is because of gullibility.\\nCan we solve this? Honestly, I‚Äôm beginning to suspect that you can‚Äôt fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\\nCode may be the best application\\nOver the course of the year, it‚Äôs become increasingly clear that writing code is one of the things LLMs are most capable of.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nIf you think about what they do, this isn‚Äôt such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\\nIt‚Äôs still astonishing to me how effective they are though.\\nOne of the great weaknesses of LLMs is their tendency to hallucinate‚Äîto imagine things that don‚Äôt correspond to reality. You would expect this to be a particularly bad problem for code‚Äîif an LLM hallucinates a method that doesn‚Äôt exist, the code should be useless.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nExcept... you can run generated code to see if it‚Äôs correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\\nHow should we feel about this as software engineers?\\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nOn the other hand, as software engineers we are better placed to take advantage of this than anyone else. We‚Äôve all been given weird coding interns‚Äîwe can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\\nThe ethics of this space remain diabolically complex\\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nJust this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading‚Äîespecially the first few pages, which lay out the issues in a way that‚Äôs surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I‚Äôve read anywhere.\\nThe legal arguments here are complex. I‚Äôm not a lawyer, but I don‚Äôt think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nLaw is not ethics. Is it OK to train models on people‚Äôs content without their permission, when those models will then be used in ways that compete with those people?\\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\\nPeople have certainly lost work to them‚Äîanecdotally, I‚Äôve seen this for copywriters, artists and translators.\\nThere are a great deal of untold stories here. I‚Äôm hoping 2024 sees significant amounts of dedicated journalism on this topic.\\nMy blog in 2023\\nHere‚Äôs a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\\nI‚Äôve written a lot about this stuff!\\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\\n\\n\\n\\nArticle\\nVisitors\\nPageviews\\n\\n\\n\\n\\nBing: ‚ÄúI will not harm you unless you harm me first‚Äù\\n1.1M\\n1.3M\\n\\n\\nLeaked Google document: ‚ÄúWe Have No Moat, And Neither Does OpenAI‚Äù\\n132k\\n162k\\n\\n\\nLarge language models are having their Stable Diffusion moment\\n121k\\n150k\\n\\n\\nPrompt injection: What‚Äôs the worst that can happen?\\n79.8k\\n95.9k\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nEmbeddings: What they are and why they matter\\n61.7k\\n79.3k\\n\\n\\nCatching up on the weird world of LLMs\\n61.6k\\n85.9k\\n\\n\\nllamafile is the new best way to run an LLM on your own computer\\n52k\\n66k\\n\\n\\nPrompt injection explained, with video, slides, and a transcript\\n51k\\n61.9k\\n\\n\\nAI-enhanced development makes me more ambitious with my projects\\n49.6k\\n60.1k\\n\\n\\nUnderstanding GPT tokenizers\\n49.5k\\n61.1k\\n\\n\\nExploring GPTs: ChatGPT in a trench coat?\\n46.4k\\n58.5k\\n\\n\\nCould you train a ChatGPT-beating model for $85,000 and run it in a browser?\\n40.5k\\n49.2k\\n\\n\\nHow to implement Q&A against your documentation with GPT3, embeddings and Datasette\\n37.3k\\n44.9k\\n\\n\\nLawyer cites fake cases invented by ChatGPT, judge is not amused\\n37.1k\\n47.4k\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nNow add a walrus: Prompt engineering in DALL-E 3\\n32.8k\\n41.2k\\n\\n\\nWeb LLM runs the vicuna-7b Large Language Model entirely in your browser, and it‚Äôs very impressive\\n32.5k\\n38.2k\\n\\n\\nChatGPT can‚Äôt access the internet, even though it really looks like it can\\n30.5k\\n34.2k\\n\\n\\nStanford Alpaca, and the acceleration of on-device large language model development\\n29.7k\\n35.7k\\n\\n\\nRun Llama 2 on your own Mac using LLM and Homebrew\\n27.9k\\n33.6k\\n\\n\\nMidjourney 5.1\\n26.7k\\n33.4k\\n\\n\\nThink of language models like ChatGPT as a ‚Äúcalculator for words‚Äù\\n25k\\n31.8k\\n\\n\\nMulti-modal prompt injection image attacks against GPT-4V\\n23.7k\\n27.4k\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI also gave a bunch of talks and podcast appearances. I‚Äôve started habitually turning my talks into annotated presentations‚Äîhere are my best from 2023:\\n\\nPrompt injection explained, with video, slides, and a transcript\\nCatching up on the weird world of LLMs\\nMaking Large Language Models work for you\\nOpen questions for AI engineering\\nEmbeddings: What they are and why they matter\\nFinancial sustainability for open source projects at GitHub Universe\\n\\nAnd in podcasts:\\n\\n\\nWhat AI can do for you on the Theory of Change\\n\\nWorking in public on Path to Citus Con\\n\\nLLMs break the internet on the Changelog\\n\\nTalking Large Language Models on Rooftop Ruby\\n\\nThoughts on the OpenAI board situation on Newsroom Robots\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content=\"Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nIndustry‚Äôs Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\\n\\n\\nPosted 31st December 2023 at 11:59 pm ¬∑ Follow me on Mastodon or Twitter or subscribe to my newsletter\\n\\n\\nMore recent articles\\n\\nRun LLMs on macOS using llm-mlx and Apple's MLX framework - 15th February 2025\\nURL-addressable Pyodide Python environments - 13th February 2025\\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\\n\\n\\n \\n\\n\\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\\n\\nPart of series LLMs annual review\\n\\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. \\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\\n\", additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nblogging\\n            68\\n\\n\\n            ai\\n            1098\\n\\n\\n            generative-ai\\n            942\\n\\n\\n            llms\\n            930\\n\\nNext: Tom Scott, and the formidable power of escalating streaks\\nPrevious: Last weeknotes of 2023\\n\\n\\n \\n \\n\\n\\nColophon\\n¬©\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n2025\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThings we learned about LLMs in 2024\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSimon Willison‚Äôs Weblog\\nSubscribe\\n\\n\\n\\n\\n\\n\\nThings we learned about LLMs in 2024\\n31st December 2024\\nA lot has happened in the world of Large Language Models over the course of 2024. Here‚Äôs a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\\nThis is a sequel to my review of 2023.\\nIn this article:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe GPT-4 barrier was comprehensively broken\\nSome of those GPT-4 models run on my laptop\\nLLM prices crashed, thanks to competition and increased efficiency\\nMultimodal vision is common, audio and video are starting to emerge\\nVoice and live camera mode are science fiction come to life\\nPrompt driven app generation is a commodity already\\nUniversal access to the best models lasted for just a few short months\\n‚ÄúAgents‚Äù still haven‚Äôt really happened yet\\nEvals really matter\\nApple Intelligence is bad, Apple‚Äôs MLX library is excellent\\nThe rise of inference-scaling ‚Äúreasoning‚Äù models\\nWas the best currently available LLM trained in China for less than $6m?\\nThe environmental impact got better\\nThe environmental impact got much, much worse\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe year of slop\\nSynthetic training data works great\\nLLMs somehow got even harder to use\\nKnowledge is incredibly unevenly distributed\\nLLMs need better criticism\\nEverything tagged ‚Äúllms‚Äù on my blog in 2024\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe GPT-4 barrier was comprehensively broken\\nIn my December 2023 review I wrote about how We don‚Äôt yet know how to build GPT-4‚ÄîOpenAI‚Äôs best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn‚Äôt?\\nI‚Äôm relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)‚Äî70 models in total.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe earliest of those was Google‚Äôs Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field‚Äîmost notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google‚Äôs Gemini series accepts up to 2 million.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nGetting back to models that beat GPT-4: Anthropic‚Äôs Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet‚Äîa model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThen there‚Äôs the rest. If you browse the Chatbot Arena leaderboard today‚Äîstill the most useful single place to get a vibes-based evaluation of models‚Äîyou‚Äôll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it‚Äôs an achievement that isn‚Äôt even particularly notable, though I personally still celebrate any time a new organization joins that list.\\nSome of those GPT-4 models run on my laptop\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It‚Äôs a powerful machine, but it‚Äôs also nearly two years old now‚Äîand crucially it‚Äôs the same laptop I‚Äôve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November‚Äîan Apache 2.0 licensed model!\\n\\nI can now run a GPT-4 class model on my laptop talks about running Meta‚Äôs Llama 3.3 70B (released in December)\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\\nThese models take up enough of my 64GB of RAM that I don‚Äôt run them often‚Äîthey don‚Äôt leave much room for anything else.\\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we‚Äôve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there‚Äôs still more to come.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nMeta‚Äôs Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it‚Äôs a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for ‚Äúa plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist‚Äù. Here‚Äôs what I got, at a respectable 20 tokens per second:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nHere‚Äôs the rest of the transcript. It‚Äôs bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\\nLLM prices crashed, thanks to competition and increased efficiency\\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\\nIn December 2023 (here‚Äôs the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\\nToday $30/mTok gets you OpenAI‚Äôs most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok‚Äînearly 7x cheaper than GPT-3.5 and massively more capable.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nOther model providers charge even less. Anthropic‚Äôs Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google‚Äôs Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok‚Äîthat‚Äôs 27x cheaper than GPT-3.5 Turbo last year.\\nI‚Äôve been tracking these pricing changes under my llm-pricing tag.\\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThere‚Äôs still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\\nHere‚Äôs a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google‚Äôs Gemini 1.5 Flash 8B (released in October), their cheapest model?\\nEach photo would need 260 input tokens and around 100 output tokens.\\n260 * 68,000 = 17,680,000 input tokens\\n17,680,000 * $0.0375/million = $0.66\\n100 * 68,000 = 6,800,000 output tokens\\n6,800,000 * $0.15/million = $1.02\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThat‚Äôs a total cost of $1.68 to process 68,000 images. That‚Äôs so absurdly cheap I had to run the numbers three times to confirm I got it right.\\nHow good are those descriptions? Here‚Äôs what I got from this command:\\nllm -m gemini-1.5-flash-8b-latest describe -a IMG_1825.jpeg\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nAgainst this photo of butterflies at the California Academy of Sciences:\\n\\n\\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that‚Äôs less than a 400th of a cent).\\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that‚Äôs what we‚Äôre getting.\\nMultimodal vision is common, audio and video are starting to emerge\\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI‚Äôs DevDay in November 2023. Google‚Äôs multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral‚Äôs Pixtral 12B and Meta‚Äôs Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\\nVoice and live camera mode are science fiction come to life\\nThe audio and live video modes that have started to emerge deserve a special mention.\\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for ‚Äúomni‚Äù) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in ‚Äú4o‚Äù mode is not running the new features yet.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I‚Äôve been using it extensively on walks with my dog and it‚Äôs amazing how much the improvement in intonation elevates the material. I‚Äôve also had a lot of fun experimenting with the OpenAI audio APIs.\\nEven more fun: Advanced Voice mode can do accents! Here‚Äôs what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nYour browser does not support the audio element.\\n\\nOpenAI aren‚Äôt the only group with a multi-modal audio model. Google‚Äôs Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that‚Äôs meant to roll out in Q1 of 2025.\\nGoogle‚Äôs NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two ‚Äúpodcast hosts‚Äù about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\\n\\n\\nYour browser does not support the audio element.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThese abilities are just a few weeks old at this point, and I don‚Äôt think their impact has been fully felt yet. If you haven‚Äôt tried them out yet you really should.\\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\\nPrompt driven app generation is a commodity already\\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)‚Äîoften in a single prompt.\\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\\nHere‚Äôs my Extract URLs app, entirely generated by Claude:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI‚Äôve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this‚ÄîGitHub Spark‚Äîin October. Mistral Chat added it as a feature called Canvas in November.\\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\\nI‚Äôve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThis prompt-driven custom interface feature is so powerful and easy to build (once you‚Äôve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\\nUniversal access to the best models lasted for just a few short months\\nFor a few short months this year all three of the best available models‚ÄîGPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro‚Äîwere freely available to most of the world.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\\nThat era appears to have ended, likely permanently, with OpenAI‚Äôs launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don‚Äôt think those days of free access to the best available models are likely to return.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n‚ÄúAgents‚Äù still haven‚Äôt really happened yet\\nI find the term ‚Äúagents‚Äù extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\\nIf you tell me that you are building ‚Äúagents‚Äù, you‚Äôve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf‚Äîthe travel agent model‚Äîand people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term ‚Äúautonomy‚Äù is often thrown into the mix too, again without including a clear definition.\\n(I also collected 211 definitions on Twitter a few months ago‚Äîhere they are in Datasette Lite‚Äîand had gemini-exp-1206 attempt to summarize them.)\\nWhatever the term may mean, agents still have that feeling of perpetually ‚Äúcoming soon‚Äù.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can‚Äôt distinguish truth from fiction?\\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie ‚ÄúEncanto 2‚Äù. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nPrompt injection is a natural consequence of this gulibility. I‚Äôve seen precious little progress on tackling that problem in 2024, and we‚Äôve been talking about it since September 2022.\\nI‚Äôm beginning to see the most popular idea of ‚Äúagents‚Äù as dependent on AGI itself. A model that‚Äôs robust against gulliblity is a very tall order indeed.\\nEvals really matter\\nAnthropic‚Äôs Amanda Askell (responsible for much of the work behind Claude‚Äôs Character):\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe boring yet crucial secret behind good system prompts is test-driven development. You don‚Äôt write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\\n\\nIt‚Äôs become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that‚Äôs most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\\nVercel‚Äôs Malte Ubl:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI‚Äôm still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them‚ÄîI‚Äôm tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\\nApple Intelligence is bad, Apple‚Äôs MLX library is excellent\\nAs a Mac user I‚Äôve been feeling a lot better about my choice of platform this year.\\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA‚Äôs CUDA over other platforms.\\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple‚Äôs MLX library, ‚Äúan array framework for Apple Silicon‚Äù. It‚Äôs fantastic.\\nApple‚Äôs mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nPrince Canuma‚Äôs excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen‚Äôs QvQ.\\nWhile MLX is a game changer, Apple‚Äôs own ‚ÄúApple Intelligence‚Äù features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nNow that those features are rolling out they‚Äôre pretty weak. As an LLM power-user I know what these models are capable of, and Apple‚Äôs LLM features offer a pale imitation of what a frontier LLM can do. Instead we‚Äôre getting notification summaries that misrepresent news headlines and writing assistant tools that I‚Äôve not found useful at all. Genmoji are kind of fun though.\\nThe rise of inference-scaling ‚Äúreasoning‚Äù models\\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI‚Äôs o1 models‚Äîinitially released as o1-preview and o1-mini on September 12th.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\\nThis is that trick where, if you get a model to talk out loud about a problem it‚Äôs solving, you often get a result which the model would not have achieved otherwise.\\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend ‚Äúreasoning tokens‚Äù thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\\nThe sequel to o1, o3 (they skipped ‚Äúo2‚Äù for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure‚ÄîI certainly don‚Äôt!‚Äîbut it appears to be a genuine next step in LLM architecture for taking on much harder problems.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\\nAlibaba‚Äôs Qwen team released their QwQ model on November 28th‚Äîunder an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nNothing yet from Anthropic or Meta but I would be very surprised if they don‚Äôt have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\\nWas the best currently available LLM trained in China for less than $6m?\\nNot quite, but almost! It does make for a great attention-grabbing headline.\\nThe big news to end the year was the release of DeepSeek v3‚Äîdropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nDeepSeek v3 is a huge 685B parameter model‚Äîone of the largest openly licensed models currently available, significantly bigger than the largest of Meta‚Äôs Llama series, Llama 3.1 405B.\\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours‚Äî11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\\nThe environmental impact got better\\nA welcome result of the increased efficiency of the models‚Äîboth the hosted ones and the ones I can run locally‚Äîis that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI think this means that, as individual users, we don‚Äôt need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That‚Äôs certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe environmental impact got much, much worse\\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There‚Äôs even talk of spinning up new nuclear power stations, but those can take decades.\\nIs this infrastructure necessary? DeepSeek v3‚Äôs $6m training cost and the continued crash in LLM prices might hint that it‚Äôs not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years‚Äô time?\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary‚Äîsometimes multiple lines from different companies serving the exact same routes!\\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK‚Äôs Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\\nThe year of slop\\n', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FSTG0bb7w73"
      },
      "source": [
        "We'll use the function to generate training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZm4CqGVzBx",
        "outputId": "65a7703a-c528-40f6-aff4-1be84902cfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe year of slop\\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nWatching in real time as ‚Äúslop‚Äù becomes a term of art. the way that ‚Äúspam‚Äù became the term for unwanted emails, ‚Äúslop‚Äù is going in the dictionary as the term for unwanted AI generated content\\n\\nI expanded that definition a tiny bit to this:\\n\\nSlop describes AI-generated content that is both unrequested and unreviewed.\\n\\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here‚Äôs what I said in the NY TImes:\\n\\nSociety needs concise ways to talk about modern A.I. ‚Äî both the positives and the negatives. ‚ÄòIgnore that email, it‚Äôs spam,‚Äô and ‚ÄòIgnore that article, it‚Äôs slop,‚Äô are both useful lessons.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI love the term ‚Äúslop‚Äù because it so succinctly captures one of the ways we should not be using generative AI!\\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\\nSynthetic training data works great\\nAn idea that surprisingly seems to have stuck in the public consciousness is that of ‚Äúmodel collapse‚Äù. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\\nThat‚Äôs clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content‚Äîdeliberately creating artificial data to help steer their models in the right way.\\nOne of the best descriptions I‚Äôve seen of this comes from the Phi-4 technical report, which included this:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives‚Äîa trick used by an increasing number of labs. DeepSeek v3 used ‚Äúreasoning‚Äù data created by DeepSeek-R1. Meta‚Äôs Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\\nLLMs somehow got even harder to use\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nA drum I‚Äôve been banging for a while is that LLMs are power-user tools‚Äîthey‚Äôre chainsaws disguised as kitchen knives. They look deceptively simple to use‚Äîhow hard can it be to type messages to a chatbot?‚Äîbut in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\\nIf anything, this problem got worse in 2024.\\nWe‚Äôve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it‚Äôs accurately reflected in the undocumented and secret training set.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems‚Äîlike Python and JavaScript and web search and image generation and maybe even database lookups... so you‚Äôd better understand what those tools are, what they can do and how to tell if the LLM used them or not.\\nDid you know ChatGPT has two entirely different ways of running Python now?\\nWant to build a Claude Artifact that talks to an external API? You‚Äôd better understand CSP and CORS HTTP headers first.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe models may have got more capable, but most of the limitations remained the same. OpenAI‚Äôs o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it‚Äôs running in. o1 can‚Äôt run web searches or use Code Interpreter, but GPT-4o can‚Äîboth in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nMeanwhile, it‚Äôs increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I‚Äôve seen so many examples of people trying to win an argument with a screenshot from ChatGPT‚Äîan inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThere‚Äôs a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can‚Äôt see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\\nKnowledge is incredibly unevenly distributed\\nMost people have heard of ChatGPT by now. How many have heard of Claude?\\n', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qUHg9sV2_y",
        "outputId": "b03bf5c6-d392-40bf-a061-1daceba2962e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\\nThe pace of change doesn‚Äôt help either. In just the past month we‚Äôve seen general availability of live interfaces where you can point your phone‚Äôs camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven‚Äôt even tried that yet.\\nGiven the ongoing (and potential) impact on society that this technology has, I don‚Äôt think the size of this gap is healthy. I‚Äôd like to see a lot more effort put into improving this.\\nLLMs need better criticism\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that ‚ÄúLLMs are useful‚Äù can be enough to kick off a huge fight.\\nI get it. There are plenty of reasons to dislike this technology‚Äîthe environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people‚Äôs jobs.\\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\\n(If you still don‚Äôt think there are any good applications at all I‚Äôm not sure why you made it to this point in the article!)\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\\nThose of us who understand this stuff have a duty to help everyone else figure it out.\\nEverything tagged ‚Äúllms‚Äù on my blog in 2024\\nBecause I undoubtedly missed a whole bunch of things, here‚Äôs every long-form post I wrote in 2024 that I tagged with llms:\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nJanuary\\n\\n7th: It‚Äôs OK to call it Artificial Intelligence\\n\\n9th: What I should have said about the term Artificial Intelligence\\n\\n17th: Talking about Open Source LLMs on Oxide and Friends\\n\\n26th: LLM 0.13: The annotated release notes\\n\\n\\n\\nFebruary\\n\\n21st: The killer app of Gemini Pro 1.5 is video\\n\\n\\n\\nMarch\\n\\n5th: Prompt injection and jailbreaking are not the same thing\\n\\n8th: The GPT-4 barrier has finally been broken\\n\\n22nd: Claude and ChatGPT for ad-hoc sidequests\\n\\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\\n\\n26th: llm cmd undo last git commit‚Äîa new plugin for LLM\\n\\n\\n\\nApril\\n\\n8th: Building files-to-prompt entirely using Claude 3 Opus\\n\\n10th: Three major LLM releases in 24 hours (plus weeknotes)\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\\n\\n22nd: Options for accessing Llama 3 from the terminal using LLM\\n\\n\\n\\nMay\\n\\n8th: Slop is the new name for unwanted AI-generated content\\n\\n15th: ChatGPT in ‚Äú4o‚Äù mode is not running the new features yet\\n\\n29th: Training is not the same as chatting: ChatGPT and other LLMs don‚Äôt remember everything you say\\n\\n\\n\\nJune\\n\\n6th: Accidental prompt injection against RAG applications\\n\\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\\n\\n17th: Language models on the command-line\\n\\n21st: Building search-based RAG using Claude, Datasette and Val Town\\n\\n27th: Open challenges for AI engineering\\n\\n\\n\\nJuly\\n\\n14th: Imitation Intelligence, my keynote for PyCon US 2024\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\\n\\n\\n\\nAugust\\n\\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\\n\\n8th: django-http-debug, a new Django app mostly written by Claude\\n\\n23rd: Claude‚Äôs API now supports CORS requests, enabling client-side applications\\n\\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\\n\\n\\n\\nSeptember\\n\\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\\n\\n10th: Notes from my appearance on the Software Misadventures Podcast\\n\\n12th: Notes on OpenAI‚Äôs new o1 chain-of-thought models\\n\\n20th: Notes on using LLMs for code\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n29th: NotebookLM‚Äôs automatically generated podcasts are surprisingly effective\\n\\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\\n\\n\\n\\nOctober\\n\\n1st: OpenAI DevDay 2024 live blog\\n\\n2nd: OpenAI DevDay: Let‚Äôs build developer tools, not digital God\\n\\n15th: ChatGPT will happily write you a thinly disguised horoscope\\n\\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\\n\\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\\n\\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\\n\\n21st: Everything I built with Claude Artifacts this week\\n\\n22nd: Initial explorations of Anthropic‚Äôs new Computer Use capability\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n24th: Notes on the new Claude analysis JavaScript code execution tool\\n\\n27th: Run a prompt to generate and execute jq programs using llm-jq\\n\\n29th: You can now run prompts against images, audio and video in your terminal using LLM\\n\\n30th: WÃ∂eÃ∂eÃ∂kÃ∂nÃ∂oÃ∂tÃ∂eÃ∂sÃ∂  Monthnotes for October\\n\\n\\n\\nNovember\\n\\n4th: Claude 3.5 Haiku\\n\\n7th: Project: VERDAD‚Äîtracking misinformation in radio broadcasts using Gemini 1.5\\n\\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\\n\\n19th: Notes from Bing Chat‚ÄîOur First Encounter With Manipulative AI\\n\\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\\n\\n\\n\\nDecember\\n\\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\\n\\n7th: Prompts.js\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content='Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\n7th: Prompts.js\\n\\n9th: I can now run a GPT-4 class model on my laptop\\n\\n10th: ChatGPT Canvas can make API requests now, but it‚Äôs complicated\\n\\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\\n\\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\\n\\n19th: Gemini 2.0 Flash ‚ÄúThinking mode‚Äù\\n\\n20th: December in LLMs has been a lot\\n\\n20th: Live blog: the 12th day of OpenAI‚Äî‚ÄúEarly evals for OpenAI o3‚Äù\\n\\n24th: Trying out QvQ‚ÄîQwen‚Äôs new visual reasoning model\\n\\n31st: Things we learned about LLMs in 2024\\n\\n\\n\\n\\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\\n', additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content=\"Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\nPosted 31st December 2024 at 6:07 pm ¬∑ Follow me on Mastodon or Twitter or subscribe to my newsletter\\n\\n\\nMore recent articles\\n\\nRun LLMs on macOS using llm-mlx and Apple's MLX framework - 15th February 2025\\nURL-addressable Pyodide Python environments - 13th February 2025\\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\\n\\n\\n \\n\\n\\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\\n\\nPart of series LLMs annual review\\n\\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. \\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. \\n\\n\\n\\n            google\\n            347\\n\\n\\n            ai\\n            1098\\n\\n\\n            openai\\n            255\\n\", additional_kwargs={}, response_metadata={})]\n",
            "Type of response: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
            "Dir of response: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'messages', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'to_messages', 'to_string', 'update_forward_refs', 'validate']\n",
            "Response: messages=[HumanMessage(content=\"Given the following context, you must generate questions based on only the provided context.\\n\\nYou are to generate 2 questions which should be provided in the following format:\\n\\n1. QUESTION #1\\n2. QUESTION #2\\n...\\n\\nContext:\\ngenerative-ai\\n            942\\n\\n\\n            llms\\n            930\\n\\n\\n            anthropic\\n            114\\n\\n\\n            gemini\\n            57\\n\\n\\n            meta\\n            26\\n\\n\\n            inference-scaling\\n            28\\n\\n\\n            long-context\\n            10\\n\\nNext: Ending a year long posting streak\\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\\n\\n\\n \\n \\n\\n\\nColophon\\n¬©\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2009\\n2010\\n2011\\n2012\\n2013\\n2014\\n2015\\n2016\\n2017\\n2018\\n2019\\n2020\\n2021\\n2022\\n2023\\n2024\\n2025\\n\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_jYOnAI43zK"
      },
      "source": [
        "### Reformating and Saving Datasets\n",
        "\n",
        "Now, we can save our datasets for later use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iF6IFFq9VsNu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
        "\n",
        "train_dataset = {\n",
        "    \"questions\" : training_questions,\n",
        "    \"relevant_contexts\" : training_relevant_contexts,\n",
        "    \"corpus\" : training_corpus\n",
        "}\n",
        "\n",
        "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(train_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PqF9WaueV-V8"
      },
      "outputs": [],
      "source": [
        "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
        "\n",
        "val_dataset = {\n",
        "    \"questions\" : val_questions,\n",
        "    \"relevant_contexts\" : val_relevant_contexts,\n",
        "    \"corpus\" : val_corpus\n",
        "}\n",
        "\n",
        "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(val_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0DSQ7WMnWAu6"
      },
      "outputs": [],
      "source": [
        "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
        "\n",
        "test_dataset = {\n",
        "    \"questions\" : test_questions,\n",
        "    \"relevant_contexts\" : test_relevant_contexts,\n",
        "    \"corpus\" : train_corpus\n",
        "}\n",
        "\n",
        "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(test_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAwklqQCgVi-"
      },
      "source": [
        "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "\n",
        "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
        "\n",
        "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
        "\n",
        "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
        "\n",
        ">> NOTE: Skip installing dependencies if you are running this notebook locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXzVHP3v1Cno",
        "outputId": "a9d6ca65-d355-460d-de89-7446a441512b"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-PGsQB7Xo6V",
        "outputId": "8df58392-a82b-45f9-ce4e-b4155522e2c6"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztG07iB8CFO"
      },
      "source": [
        "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
        "\n",
        "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B-WbpuUWYFJr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import InputExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJtPPlck8HBE"
      },
      "source": [
        "We're using a toy batch size here to reflect the limited number of examples we have.\n",
        "\n",
        "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8Lokhy6KYHAv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-6DT8hc8PmT"
      },
      "source": [
        "Let's move our dataset into the expected format for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JJk37zQsYJ4P"
      },
      "outputs": [],
      "source": [
        "corpus = train_dataset['corpus']\n",
        "queries = train_dataset['questions']\n",
        "relevant_docs = train_dataset['relevant_contexts']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    doc_id = relevant_docs[query_id][0]\n",
        "    text = corpus[doc_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFx7KHI8TL0"
      },
      "source": [
        "Now we can create a `torch` `DataLoader`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tiizmeIqZ_-w"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vA8rzlX8XbT"
      },
      "source": [
        "Next up, we'll prepare our loss function!\n",
        "\n",
        "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
        "\n",
        "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
        "\n",
        "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Uga4nnBqlVeh"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJG4fOm66PHI"
      },
      "source": [
        "##### üèóÔ∏è Activity #2:\n",
        "\n",
        "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
        "\n",
        "Why are these losses specifically doing? Please write a short summary of each loss.\n",
        "\n",
        "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚úÖ Answer #2:\n",
        "\n",
        "```python\n",
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")\n",
        "```\n",
        "\n",
        "This code implements a nested (Matryoshka) loss structure:\n",
        "\n",
        "1. **MultipleNegativesRankingLoss (Inner Loss)**\n",
        "```python\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "```\n",
        "- Primary loss function that:\n",
        "  - Takes a batch of sentence pairs\n",
        "  - For each anchor sentence (question):\n",
        "    - One positive pair (correct context)\n",
        "    - Multiple negative pairs (other contexts in batch)\n",
        "  - Optimizes to maximize similarity with positive pair\n",
        "  - Minimizes similarity with negative pairs\n",
        "  - Uses cross-entropy loss under the hood\n",
        "\n",
        "2. **MatryoshkaLoss (Outer Loss)**\n",
        "```python\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")\n",
        "```\n",
        "- Wraps the inner loss in a multi-dimensional structure\n",
        "- Creates nested embeddings of different sizes:\n",
        "  - Full dimension: 768 (base embedding)\n",
        "  - Reduced dimensions: 512 ‚Üí 256 ‚Üí 128 ‚Üí 64\n",
        "- Benefits:\n",
        "  - Flexibility: Can use different embedding sizes for different tasks\n",
        "  - Efficiency: Smaller dimensions for resource-constrained environments\n",
        "  - Performance: Maintains quality across different dimensions\n",
        "\n",
        "The dimensions work like nested dolls (hence \"Matryoshka\"):\n",
        "```\n",
        "[768] ‚Üí Contains all information\n",
        "  [512] ‚Üí Compressed but still detailed\n",
        "    [256] ‚Üí Medium compression\n",
        "      [128] ‚Üí Higher compression\n",
        "        [64] ‚Üí Most compressed\n",
        "```\n",
        "\n",
        "This setup allows:\n",
        "1. Training one model that can output embeddings of multiple sizes\n",
        "2. Maintaining performance across different dimensionality requirements\n",
        "3. Flexibility in deployment (can choose size based on resources/needs)\n",
        "4. Efficient storage and computation options\n",
        "\n",
        "This is particularly useful for:\n",
        "- Resource-constrained environments (can use smaller dimensions)\n",
        "- Systems requiring different precision levels\n",
        "- Balancing performance vs computational cost\n",
        "- Production deployments where flexibility is needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKxRuXfH844c"
      },
      "source": [
        "Now we can set-up our evaluator.\n",
        "\n",
        "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "f0hAFwUyaHQG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "corpus = val_dataset['corpus']\n",
        "queries = val_dataset['questions']\n",
        "relevant_docs = val_dataset['relevant_contexts']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfap_ct8-bU"
      },
      "source": [
        "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "svZG0pBHiQr6"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxitWoNX9DwW"
      },
      "source": [
        "It's training time!\n",
        "\n",
        "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/b1ez2i2n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f57e83ea120>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753,
          "referenced_widgets": [
            "0d3fc6edfdab4fe9aff8e805632eadad",
            "ebe8aa7a82124b57bce2d826c1dea0fa",
            "aa11c10b4234452d9430744ab89b59a2",
            "ca05cbcd72cb41d9856804ffb17b26cb",
            "cc2a33e9a7ac4c5699346fcbf53b7c95",
            "408f4dfce21a45cfad36047677ec8658",
            "ca5804644ef345c1b4f670fb7f088fe8",
            "2a872283afaa4a33a9bc3f9e57b3650b",
            "fe4d1052824c4ed29c38c5311087e650",
            "e283b1608c4d4266a03c57dc95aabb2e",
            "bfc4997e3bd94e66bebaa1ffdae1b99e"
          ]
        },
        "id": "aDhUHZY-iR09",
        "outputId": "6dbd9320-f7b9-46d6-b891-efdd12086631"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c7f385d2d874f4a95215205fbaed24b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [170/170 1:46:42, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cosine Accuracy@1</th>\n",
              "      <th>Cosine Accuracy@3</th>\n",
              "      <th>Cosine Accuracy@5</th>\n",
              "      <th>Cosine Accuracy@10</th>\n",
              "      <th>Cosine Precision@1</th>\n",
              "      <th>Cosine Precision@3</th>\n",
              "      <th>Cosine Precision@5</th>\n",
              "      <th>Cosine Precision@10</th>\n",
              "      <th>Cosine Recall@1</th>\n",
              "      <th>Cosine Recall@3</th>\n",
              "      <th>Cosine Recall@5</th>\n",
              "      <th>Cosine Recall@10</th>\n",
              "      <th>Cosine Ndcg@10</th>\n",
              "      <th>Cosine Mrr@10</th>\n",
              "      <th>Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.092000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.701742</td>\n",
              "      <td>0.637159</td>\n",
              "      <td>0.644128</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='finetuned_arctic_ft',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "58699484312f460cb96ac44e3af14aa2",
            "d45a7d95e5b14a6f88d5798fec9c40a7",
            "b011a6ccf8e745c6be6f3a17b7d61dce",
            "1016780729b04ba489d488922173ddae",
            "9d34dac405fd40139d5dc04eecef55ed",
            "d6169577ffb341a69eb0175e301b6a44",
            "d4acc9a7aca04564bfaefe33de079394",
            "4e2c257232c3472697e03350de43cb30",
            "381c780ce17e4662981175400fb8a0f8",
            "878dc96f87dd45f389948a546db33e94",
            "006bf6f5ebaf400486a6b82610381db0",
            "9198dd0fa8f04aa7b75307aa2d513bfb",
            "59cd26ae53024fbd85431b6683cd119c",
            "4e7ccd97042c4fb9a201b6dc76762e04",
            "72ec09e2cbbf4788b1561abfcdd0819a",
            "fb1e19624fde4d3c8048ce00264d6056",
            "9638a0456e0c41b1b9b9757932f55c53",
            "18825dc83221412ab602830fc00db71b",
            "a6ea48a80c194d128959422368aa0e10",
            "9f4026c62c60493caa18c014ae414e65"
          ]
        },
        "id": "b3iwclvyRD8L",
        "outputId": "1471e984-9351-478c-de34-6eb32263fa30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4453e6855844dd5876e1e1219443860",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_pn-Y6yjRoHk"
      },
      "outputs": [],
      "source": [
        "hf_username = \"dataera2013\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e5c86e0e33264ed8b6325e44f9421653",
            "acf3991e5d644f468264f561b28b514a",
            "4a723c6a56e94309b2e3abe63f28aedc",
            "ef25768d3b0746b48c6d02e9bd151bf9",
            "cc1f0bc4a1a74568b9c74a7392a1568f",
            "95160a05de5b402b9bdb0bd2d099cd00",
            "cf376b0ea3544055b8867d7013526502",
            "869814ecd49e46f9a33dd53130e6953f",
            "cc7d460d3c5a4ac6a9b64929736d6598",
            "13e9bf583f6442d395334947379a280a",
            "1995b4d98fe044e38f1980d47033ca40"
          ]
        },
        "id": "Nqhf3zWa9AiJ",
        "outputId": "c601b2a8-f8e9-4d71-9c7b-8ea4999ff077"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f778e672d93c4fa0a3340dec9ca4ff43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/dataera2013/legal-ft-2/commit/c0abc78c3ccd6e30c2d427c81e0dc96a6accdcb1'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.push_to_hub(f\"{hf_username}/legal-ft-2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bo0zW5k9Poq"
      },
      "source": [
        "## Task 5: Evaluating our Retriever\n",
        "\n",
        "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
        "\n",
        "We'll start with some basic imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Vq-2oqU0wHFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jD0qrIh9X8f"
      },
      "source": [
        "Now we'll define a function that will help us evaluate our retrieval process.\n",
        "\n",
        "> NOTE: We're assuming 1 correct document in a \"hit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0713_3cowX4q"
      },
      "outputs": [],
      "source": [
        "def evaluate_openai(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "  corpus = dataset['corpus']\n",
        "  questions = dataset['questions']\n",
        "  relevant_docs = dataset['relevant_contexts']\n",
        "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
        "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "  eval_results = []\n",
        "  for id, question in tqdm(questions.items()):\n",
        "    retrieved_nodes = retriever.invoke(question)\n",
        "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
        "    expected_id = relevant_docs[id][0]\n",
        "    is_hit = expected_id in retrieved_ids\n",
        "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
        "\n",
        "  return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOr49m4O9lxY"
      },
      "source": [
        "All that's left to do is evaluate, we'll evaluate our model against:\n",
        "\n",
        "1. OpenAI's closed source `text-embedding-3-small`\n",
        "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
        "\n",
        "Let's see how it stacks up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaeYpf593IW"
      },
      "source": [
        "### `text-embedding-3-small`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyY3PztaxnU3",
        "outputId": "5a1ec5e9-00fc-4140-d5b6-aa752dd4c9fa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8173a667458245ee82e41f27a97161d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "te3_results = evaluate_openai(test_dataset, te3_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kkyW90TCxx_i"
      },
      "outputs": [],
      "source": [
        "te3_results_df = pd.DataFrame(te3_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscVRdNCylJ-",
        "outputId": "275beff8-3c59-4063-8270-c01736b4ee05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.7083333333333334)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ra-mh0L96dQ"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEskxwvFypHe",
        "outputId": "a3aad8ce-48ef-4d8f-9ed0-122b1ec9a000"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a71125641014655a6f1162195dc4003",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
        "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KlKgiXTWzMTg"
      },
      "outputs": [],
      "source": [
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>expected_id</th>\n",
              "      <th>is_hit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3229c051-3f54-4104-9644-3a014d51a0b9</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>3f516b02-c270-440c-889e-d755a57b3612</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68ba016c-59cc-45a4-a3e0-aaf416376329</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\nThe knowledge ga...</td>\n",
              "      <td>3f516b02-c270-440c-889e-d755a57b3612</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7152573e-aeed-46c2-891b-755bb9fb2740</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>82ecb6f2-efc0-4661-a417-6208a207da63</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dcec525e-d085-47d2-98d9-bee0ed289045</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\nA lot of people ...</td>\n",
              "      <td>82ecb6f2-efc0-4661-a417-6208a207da63</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4064a412-791c-4a5f-8696-d66b5f869844</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>565ad4c9-5058-4707-ac06-4e745c7b9f65</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>e1552679-8e47-4101-9184-331eb722fee8</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\nI like people wh...</td>\n",
              "      <td>565ad4c9-5058-4707-ac06-4e745c7b9f65</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>42bdd36d-867f-401c-a753-b1ab1fbb0b69</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>8a5d1b36-a6c0-4a9a-95d2-e833d1f21c44</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a670dfaa-b62e-4bbe-b859-c98a0b19e2a2</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\nI think telling ...</td>\n",
              "      <td>8a5d1b36-a6c0-4a9a-95d2-e833d1f21c44</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>868155ee-e75a-4d65-a493-13023590dec0</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>7863fe22-8bcf-47f0-8478-5d925f422c1f</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>a3e3d26d-1184-4c6b-bbdd-7687f2317c37</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\nJanuary\\n\\n7th: ...</td>\n",
              "      <td>7863fe22-8bcf-47f0-8478-5d925f422c1f</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>dc5da9ec-d4fa-4dac-a49a-a5ef5ec782b1</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>712b2692-29c8-4695-9dcc-f5e0f4d6a940</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>212190e5-d665-4014-90db-f69cd0ce2d60</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\n17th: AI for Dat...</td>\n",
              "      <td>712b2692-29c8-4695-9dcc-f5e0f4d6a940</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>d293578d-d1d2-42f5-b0be-4bad97846e60</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>e94c22e5-40dc-438a-a598-411d31277b33</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>55229bf5-02f4-413e-8cdb-8f9f873ca170</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\n19th: Weeknotes:...</td>\n",
              "      <td>e94c22e5-40dc-438a-a598-411d31277b33</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>caae0f03-d9d8-4a3c-b20f-43803da57946</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>a9ff7e82-075c-4bbe-b0f9-43dbbb61c853</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>695a6325-6b7c-4d9b-9d7a-ee3cb077005e</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\n29th: NotebookLM...</td>\n",
              "      <td>a9ff7e82-075c-4bbe-b0f9-43dbbb61c853</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>cb420a21-4f19-424d-b0f7-990f59d42daf</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>dd6f5c04-af7c-4cc7-9635-f0ff67c78e4f</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>81f3f642-4af7-4bde-a78f-6935942ebc14</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\n24th: Notes on t...</td>\n",
              "      <td>dd6f5c04-af7c-4cc7-9635-f0ff67c78e4f</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7db85473-e33c-48e8-88aa-685bda6b2e8f</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>d1bd8b61-88f1-435b-9a7a-2f3ca71ec069</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>00fc5463-2093-4340-9de6-3f1de346e382</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\n7th: Prompts.js\\...</td>\n",
              "      <td>d1bd8b61-88f1-435b-9a7a-2f3ca71ec069</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>8a7a99ca-957d-46c3-ad3b-b600b5e2108c</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>4f33d6f2-a9cf-4b03-abbc-e50e000e2329</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>6b766a66-2333-44d8-87cd-77c2c8aa3e17</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\nPosted 31st Dece...</td>\n",
              "      <td>4f33d6f2-a9cf-4b03-abbc-e50e000e2329</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>43f37e45-234e-41f2-ab08-30a8d9b2e30d</td>\n",
              "      <td>QUESTION #1\\n</td>\n",
              "      <td>3ee032da-0ca8-477f-abc9-f3797ace452e</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>75d66338-2f06-4aa1-8f1f-cf7707335be6</td>\n",
              "      <td>QUESTION #2\\n...\\n\\nContext:\\ngenerative-ai\\n ...</td>\n",
              "      <td>3ee032da-0ca8-477f-abc9-f3797ace452e</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      id  \\\n",
              "0   3229c051-3f54-4104-9644-3a014d51a0b9   \n",
              "1   68ba016c-59cc-45a4-a3e0-aaf416376329   \n",
              "2   7152573e-aeed-46c2-891b-755bb9fb2740   \n",
              "3   dcec525e-d085-47d2-98d9-bee0ed289045   \n",
              "4   4064a412-791c-4a5f-8696-d66b5f869844   \n",
              "5   e1552679-8e47-4101-9184-331eb722fee8   \n",
              "6   42bdd36d-867f-401c-a753-b1ab1fbb0b69   \n",
              "7   a670dfaa-b62e-4bbe-b859-c98a0b19e2a2   \n",
              "8   868155ee-e75a-4d65-a493-13023590dec0   \n",
              "9   a3e3d26d-1184-4c6b-bbdd-7687f2317c37   \n",
              "10  dc5da9ec-d4fa-4dac-a49a-a5ef5ec782b1   \n",
              "11  212190e5-d665-4014-90db-f69cd0ce2d60   \n",
              "12  d293578d-d1d2-42f5-b0be-4bad97846e60   \n",
              "13  55229bf5-02f4-413e-8cdb-8f9f873ca170   \n",
              "14  caae0f03-d9d8-4a3c-b20f-43803da57946   \n",
              "15  695a6325-6b7c-4d9b-9d7a-ee3cb077005e   \n",
              "16  cb420a21-4f19-424d-b0f7-990f59d42daf   \n",
              "17  81f3f642-4af7-4bde-a78f-6935942ebc14   \n",
              "18  7db85473-e33c-48e8-88aa-685bda6b2e8f   \n",
              "19  00fc5463-2093-4340-9de6-3f1de346e382   \n",
              "20  8a7a99ca-957d-46c3-ad3b-b600b5e2108c   \n",
              "21  6b766a66-2333-44d8-87cd-77c2c8aa3e17   \n",
              "22  43f37e45-234e-41f2-ab08-30a8d9b2e30d   \n",
              "23  75d66338-2f06-4aa1-8f1f-cf7707335be6   \n",
              "\n",
              "                                             question  \\\n",
              "0                                       QUESTION #1\\n   \n",
              "1   QUESTION #2\\n...\\n\\nContext:\\nThe knowledge ga...   \n",
              "2                                       QUESTION #1\\n   \n",
              "3   QUESTION #2\\n...\\n\\nContext:\\nA lot of people ...   \n",
              "4                                       QUESTION #1\\n   \n",
              "5   QUESTION #2\\n...\\n\\nContext:\\nI like people wh...   \n",
              "6                                       QUESTION #1\\n   \n",
              "7   QUESTION #2\\n...\\n\\nContext:\\nI think telling ...   \n",
              "8                                       QUESTION #1\\n   \n",
              "9   QUESTION #2\\n...\\n\\nContext:\\nJanuary\\n\\n7th: ...   \n",
              "10                                      QUESTION #1\\n   \n",
              "11  QUESTION #2\\n...\\n\\nContext:\\n17th: AI for Dat...   \n",
              "12                                      QUESTION #1\\n   \n",
              "13  QUESTION #2\\n...\\n\\nContext:\\n19th: Weeknotes:...   \n",
              "14                                      QUESTION #1\\n   \n",
              "15  QUESTION #2\\n...\\n\\nContext:\\n29th: NotebookLM...   \n",
              "16                                      QUESTION #1\\n   \n",
              "17  QUESTION #2\\n...\\n\\nContext:\\n24th: Notes on t...   \n",
              "18                                      QUESTION #1\\n   \n",
              "19  QUESTION #2\\n...\\n\\nContext:\\n7th: Prompts.js\\...   \n",
              "20                                      QUESTION #1\\n   \n",
              "21  QUESTION #2\\n...\\n\\nContext:\\nPosted 31st Dece...   \n",
              "22                                      QUESTION #1\\n   \n",
              "23  QUESTION #2\\n...\\n\\nContext:\\ngenerative-ai\\n ...   \n",
              "\n",
              "                             expected_id  is_hit  \n",
              "0   3f516b02-c270-440c-889e-d755a57b3612    True  \n",
              "1   3f516b02-c270-440c-889e-d755a57b3612    True  \n",
              "2   82ecb6f2-efc0-4661-a417-6208a207da63    True  \n",
              "3   82ecb6f2-efc0-4661-a417-6208a207da63    True  \n",
              "4   565ad4c9-5058-4707-ac06-4e745c7b9f65    True  \n",
              "5   565ad4c9-5058-4707-ac06-4e745c7b9f65    True  \n",
              "6   8a5d1b36-a6c0-4a9a-95d2-e833d1f21c44   False  \n",
              "7   8a5d1b36-a6c0-4a9a-95d2-e833d1f21c44    True  \n",
              "8   7863fe22-8bcf-47f0-8478-5d925f422c1f   False  \n",
              "9   7863fe22-8bcf-47f0-8478-5d925f422c1f    True  \n",
              "10  712b2692-29c8-4695-9dcc-f5e0f4d6a940    True  \n",
              "11  712b2692-29c8-4695-9dcc-f5e0f4d6a940    True  \n",
              "12  e94c22e5-40dc-438a-a598-411d31277b33   False  \n",
              "13  e94c22e5-40dc-438a-a598-411d31277b33    True  \n",
              "14  a9ff7e82-075c-4bbe-b0f9-43dbbb61c853   False  \n",
              "15  a9ff7e82-075c-4bbe-b0f9-43dbbb61c853    True  \n",
              "16  dd6f5c04-af7c-4cc7-9635-f0ff67c78e4f   False  \n",
              "17  dd6f5c04-af7c-4cc7-9635-f0ff67c78e4f    True  \n",
              "18  d1bd8b61-88f1-435b-9a7a-2f3ca71ec069   False  \n",
              "19  d1bd8b61-88f1-435b-9a7a-2f3ca71ec069    True  \n",
              "20  4f33d6f2-a9cf-4b03-abbc-e50e000e2329   False  \n",
              "21  4f33d6f2-a9cf-4b03-abbc-e50e000e2329    True  \n",
              "22  3ee032da-0ca8-477f-abc9-f3797ace452e    True  \n",
              "23  3ee032da-0ca8-477f-abc9-f3797ace452e    True  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arctic_embed_m_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV5vJWrJzOhc",
        "outputId": "30049be6-deb9-4ceb-dc13-24d7e125f131"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.7083333333333334)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcR3-0s19_lu"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilse1LduzP1i",
        "outputId": "292ab58f-7594-45fc-a5b8-1062cc553f66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a5725b226204b44b463bc7e27435d8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
        "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "xxhZPqkNzZlh"
      },
      "outputs": [],
      "source": [
        "finetune_results_df = pd.DataFrame(finetune_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thAK2BXzaj6",
        "outputId": "e890b5d1-86b7-4bfe-8ffe-a779a132e0c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.7083333333333334)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
        "finetune_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegFM209mBk3"
      },
      "source": [
        "## Task 1: Vibe Checking the RAG Pipeline\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzg0AA5krgR4"
      },
      "source": [
        "### Creating New Chunks\n",
        "\n",
        "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KwQ2_LqNr0Tw"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIdxahHXpP-c"
      },
      "source": [
        "### Base Chain\n",
        "\n",
        "We'll start by constructing our base chain, which will use the untrained retrieval model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOsxIXpNpWC2"
      },
      "source": [
        "#### R - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "azIGIKYfmNCT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
        "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1nVZ0KpX5N"
      },
      "source": [
        "#### A - Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "G10Fr-aKojeA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euq6RQEopZvD"
      },
      "source": [
        "#### G - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5-mfbbrypMHG"
      },
      "outputs": [],
      "source": [
        "rag_llm =  ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2p4mnUpbYY"
      },
      "source": [
        "#### RAG - LCEL RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ssuR-LaboyGq"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "base_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "emm6WbB9pfKt",
        "outputId": "f0e0c83f-c617-493e-99ce-869061a315f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An agent, in the context of AI, is an infuriatingly vague term that generally refers to AI systems that can act on your behalf. There are two main interpretations: one sees agents as systems that go and act for you (like a travel agent), while the other views them as LLMs (large language models) that have access to tools and can run them in a loop to solve problems. However, the term lacks a clear and widely understood definition, leading to confusion about its meaning and utility.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "mUOrd0OBprAq",
        "outputId": "0070d677-0bde-48be-a8a3-f0c53b9eee90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OnfuFl59py7I",
        "outputId": "37480e29-17a6-40e2-fe8a-7eb6ea270569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-NmqwHBDqTZ8",
        "outputId": "86d7b59a-97c6-4b65-805f-ae449e3b1a20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNS0UJAp3lC"
      },
      "source": [
        "### Fine-tuned Embedding Model\n",
        "\n",
        "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ihO7tP6mqATy"
      },
      "outputs": [],
      "source": [
        "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
        "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "1_cIFvWzqKGY"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OJmRHJF2qNgj",
        "outputId": "b99d4b58-8487-48ec-ee6c-8ea93f9b0192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An \"Agent\" is a term that lacks a single, clear, and widely understood meaning in the context of AI. It is often used to refer to AI systems that can act on behalf of a user, but there are various interpretations of what this entails. Some people view agents as systems that go away and perform tasks autonomously, while others think of them as LLMs (Large Language Models) that have access to tools and can run processes in a loop to solve problems. The term is often associated with concepts like autonomy, but there is no consensus on its definition or practical implementation, leading to skepticism about their utility.'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "EnK-c2ugqPPh",
        "outputId": "b8300e0a-1b51-48dd-93c3-254e0aa84e36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "83hssg1AWozc",
        "outputId": "8d1ef134-7889-4379-a49b-6c0a476b7a1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rsHmGeFbqRET",
        "outputId": "1ab223c2-d2fc-46ac-8541-af038de3166d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The largest model that Simon has run on his phone is the Llama 3.2 3B model.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgD8seY_I3W"
      },
      "source": [
        "####‚ùìQuestion #2:\n",
        "\n",
        "Which LCEL RAG Chain do you think answered the questions better, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚úÖ Answer #2:\n",
        "\n",
        "Now with the complete responses, I can make a much better comparison. The fine-tuned model's RAG chain performed better overall. Here's the detailed analysis:\n",
        "\n",
        "1. \"What is an Agent?\":\n",
        "   - Fine-tuned model gave a more nuanced and complete answer, including:\n",
        "     - Explicitly stated the lack of clear definition\n",
        "     - Covered both interpretations more thoroughly\n",
        "     - Mentioned the concept of autonomy\n",
        "     - Addressed the skepticism about utility\n",
        "   - Base model's answer was more concise but missed some important nuances\n",
        "\n",
        "2. \"Who has produced better models than GPT-3?\":\n",
        "   - Both gave identical answers, listing the same organizations\n",
        "   - This shows consistency in factual retrieval for straightforward questions\n",
        "\n",
        "3. \"What is the laziest AI month?\":\n",
        "   - Both correctly responded \"I do not know\"\n",
        "   - This shows good handling of questions that can't be answered from the context\n",
        "\n",
        "4. \"What is the largest model that Simon has run on his phone?\":\n",
        "   - Fine-tuned model correctly identified \"Llama 3.2 3B model\"\n",
        "   - Base model incorrectly responded \"I do not know\" despite the information being present in the context\n",
        "   - This is a significant difference showing better retrieval and context understanding by the fine-tuned model\n",
        "\n",
        "The fine-tuned model's RAG chain performed better because:\n",
        "1. More detailed and nuanced responses for complex concepts (Agent definition)\n",
        "2. Better retrieval of specific information (Llama 3.2 3B model)\n",
        "3. Maintained the same accuracy on factual questions\n",
        "4. Kept the appropriate \"I do not know\" responses for unanswerable questions\n",
        "\n",
        "The fine-tuning appears to have improved the model's ability to:\n",
        "- Extract relevant information from context more reliably\n",
        "- Provide more comprehensive answers while maintaining accuracy\n",
        "- Better handle both conceptual and factual questions\n",
        "\n",
        "This suggests the fine-tuning process successfully improved the model's retrieval and response generation capabilities while maintaining its ability to acknowledge limitations when appropriate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbq1sZArIx4"
      },
      "source": [
        "## Task 2: RAGAS Evaluation\n",
        "\n",
        "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
        "\n",
        "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     /home/nageshbm/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "# First, install all required NLTK data\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "# Download all NLTK data\n",
        "nltk.download('all')\n",
        "\n",
        "# Now proceed with loading\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now proceed with loading the HTML files\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "893b0d63bdcf4949890aba11fe672d04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f1340b9d24a479ebe9bd040ae242ecc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af8c9dd343844573bc593e2085a845d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7e44e4516fe4d24bc83475dc55a8168",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dd79d3d7e2245c4b4c767625b0dd1ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8f8aab904f64ff3a8675bff62c0cb9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e000a96da42418fafd38963062b2bfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d649c0f67b244140b835e7451951b4b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0cb1171de2a4b27a46d1e7337313fd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wut iz Meta's role in the development of LLMs?</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>Meta has contributed to the development of LLM...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What happen in September with prompt injection?</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who is Simon Willson?</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison is the author of a weblog that ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What insights has Simon Willison shared about ...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>Simon Willison has discussed the profound impa...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the environmental impact of AI models ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How did the advancements in GPT-4 and the subs...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The advancements in GPT-4 and subsequent techn...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How have advancements in energy efficiency and...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Advancements in energy efficiency and the envi...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How has the environmental impact of AI models ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What were some key advancements in large langu...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison‚Äôs Weblog Subscribe ...</td>\n",
              "      <td>In 2023, large language models (LLMs) saw sign...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How has Google's Gemini model contributed to t...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\ngets you OpenAI‚Äôs most expensive m...</td>\n",
              "      <td>Google's Gemini model, particularly the Gemini...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How has the development of Claude 3 and its su...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The development of Claude 3 and its subsequent...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How has Google's Gemini 1.5 Pro contributed to...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Google's Gemini 1.5 Pro, released in February,...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0      Wut iz Meta's role in the development of LLMs?   \n",
              "1     What happen in September with prompt injection?   \n",
              "2                               Who is Simon Willson?   \n",
              "3   What insights has Simon Willison shared about ...   \n",
              "4   How has the environmental impact of AI models ...   \n",
              "5   How did the advancements in GPT-4 and the subs...   \n",
              "6   How have advancements in energy efficiency and...   \n",
              "7   How has the environmental impact of AI models ...   \n",
              "8   What were some key advancements in large langu...   \n",
              "9   How has Google's Gemini model contributed to t...   \n",
              "10  How has the development of Claude 3 and its su...   \n",
              "11  How has Google's Gemini 1.5 Pro contributed to...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison‚Äôs Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "5   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "6   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "7   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "8   [<1-hop>\\n\\nSimon Willison‚Äôs Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\ngets you OpenAI‚Äôs most expensive m...   \n",
              "10  [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "11  [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Meta has contributed to the development of LLM...   \n",
              "1   In September last year, the term 'prompt injec...   \n",
              "2   Simon Willison is the author of a weblog that ...   \n",
              "3   Simon Willison has discussed the profound impa...   \n",
              "4   The environmental impact of AI models has impr...   \n",
              "5   The advancements in GPT-4 and subsequent techn...   \n",
              "6   Advancements in energy efficiency and the envi...   \n",
              "7   The environmental impact of AI models has impr...   \n",
              "8   In 2023, large language models (LLMs) saw sign...   \n",
              "9   Google's Gemini model, particularly the Gemini...   \n",
              "10  The development of Claude 3 and its subsequent...   \n",
              "11  Google's Gemini 1.5 Pro, released in February,...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 267,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67d91a4eb31e44c1952330cd858f8eb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aee6462fd00e46a58ccf020f2f4b719f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48eed11d64cc4cc89fec2fd0aaea198a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/31.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c6e2e395d9940f4b45547e80da55cf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ebd868a25464426a63a12fb6d574312",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fce145028a84ed896f4e26c21aed4c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dataera2013/legal-ft-2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5403880d421497dbad6dc974e95b7f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b102d01df99843f6b87bcefd86c1feb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4382a36c61e74cbcb4061a6b5f813fcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dcf999cb63f4a72b3a401d1493c8c65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa5d2e0677ac407da39449c7913d6500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling%2Fconfig.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ft_embeddings = HuggingFaceEmbeddings(model_name=\"dataera2013/legal-ft-2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"base_ai_across_years\",\n",
        "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"ft_ai_across_years\",\n",
        "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "base_vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"base_ai_across_years\",\n",
        "    embedding=base_embeddings,\n",
        ")\n",
        "\n",
        "ft_vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"ft_ai_across_years\",\n",
        "    embedding=ft_embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "execution_count": 272,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_documents = text_splitter.split_documents(docs)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = base_vector_store.add_documents(documents=split_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {},
      "outputs": [],
      "source": [
        "__ = ft_vector_store.add_documents(documents=split_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_retriever = base_vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "ft_retriever = ft_vector_store.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [],
      "source": [
        "def base_retrieve(state):\n",
        "  retrieved_docs = base_retriever.invoke(state[\"question\"])\n",
        "  return {\"context\" : retrieved_docs}\n",
        "\n",
        "def ft_retrieve(state): \n",
        "  retrieved_docs = ft_retriever.invoke(state[\"question\"])\n",
        "  return {\"context\" : retrieved_docs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
        "\n",
        "### Question\n",
        "{question}\n",
        "\n",
        "### Context\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(state):\n",
        "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "  messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
        "  response = llm.invoke(messages)\n",
        "  return {\"response\" : response.content}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class BaseState(TypedDict):\n",
        "  question: str\n",
        "  context: List[Document]\n",
        "  response: str\n",
        "\n",
        "class FState(TypedDict):\n",
        "  question: str\n",
        "  context: List[Document]\n",
        "  response: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_graph_builder = StateGraph(BaseState).add_sequence([base_retrieve, generate])\n",
        "base_graph_builder.add_edge(START, \"base_retrieve\")\n",
        "base_graph = base_graph_builder.compile()\n",
        "\n",
        "ft_graph_builder = StateGraph(FState).add_sequence([ft_retrieve, generate])\n",
        "ft_graph_builder.add_edge(START, \"ft_retrieve\")\n",
        "ft_graph = ft_graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LLM agents are useful primarily in their capacity to assist with complex tasks like writing code. They excel at understanding the simpler grammar rules associated with programming languages, making them effective tools for coding-related tasks. Additionally, they can aid in generating training data for smaller models, which enhances the development of AI systems.\\n\\nDespite their flaws and inherent unreliability, LLMs present valuable opportunities when used correctly. The key lies in understanding how to leverage their capabilities while being aware of their limitations. There are good applications for LLMs, provided users acquire the necessary skills to navigate their complexities effectively.'"
            ]
          },
          "execution_count": 285,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = base_graph.invoke({\"question\" : \"How are LLM agents useful?\"})\n",
        "response[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LLM agents are useful in several ways, as highlighted in the provided context:\\n\\n1. **Productivity Improvement**: LLMs can enhance personal productivity by performing tasks such as answering questions, summarizing documents, translating languages, extracting information, and writing code. This allows users to accomplish more in less time.\\n\\n2. **Guidance for Effective Use**: Though LLMs have the potential to create value, users often need guidance to navigate their complexities and avoid pitfalls. This guidance helps individuals to effectively implement LLMs in ways that can improve their quality of life.\\n\\n3. **Positive Applications**: Despite criticisms, the context indicates that there are genuinely good applications for LLMs. Identifying and promoting these applications can help mitigate the negative aspects associated with LLMs, such as environmental impact and potential misuse.\\n\\n4. **Aid in Training Data Creation**: Larger LLMs can assist in generating training data for smaller and cheaper alternatives, which showcases another valuable application in the development and fine-tuning of AI models.\\n\\nOverall, LLM agents can be beneficial tools, but they require a careful approach to use effectively and responsibly.'"
            ]
          },
          "execution_count": 286,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = ft_graph.invoke({\"question\" : \"How are LLM agents useful?\"})\n",
        "response[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_dataset = dataset\n",
        "ft_dataset = dataset\n",
        "\n",
        "for base_test_row in base_dataset:\n",
        "  base_response = base_graph.invoke({\"question\" : base_test_row.eval_sample.user_input})\n",
        "  base_test_row.eval_sample.response = base_response[\"response\"]\n",
        "  base_test_row.eval_sample.retrieved_contexts = [context.page_content for context in base_response[\"context\"]]\n",
        "\n",
        "for ft_test_row in ft_dataset:\n",
        "  ft_response = ft_graph.invoke({\"question\" : ft_test_row.eval_sample.user_input})\n",
        "  ft_test_row.eval_sample.response = ft_response[\"response\"]\n",
        "  ft_test_row.eval_sample.retrieved_contexts = [context.page_content for context in ft_response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wut iz Meta's role in the development of LLMs?</td>\n",
              "      <td>[I wrote about how Large language models are h...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>Meta's role in the development of Large Langua...</td>\n",
              "      <td>Meta has contributed to the development of LLM...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What happen in September with prompt injection?</td>\n",
              "      <td>[But on the other hand, the things you sometim...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>In September of the previous year, the term \"p...</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who is Simon Willson?</td>\n",
              "      <td>[Posted \\n\\n31st December 2023 at 11:59 pm ¬∑ F...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison is a writer and blogger who foc...</td>\n",
              "      <td>Simon Willison is the author of a weblog that ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What insights has Simon Willison shared about ...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>Simon Willison has shared several insights reg...</td>\n",
              "      <td>Simon Willison has discussed the profound impa...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the environmental impact of AI models ...</td>\n",
              "      <td>[The much bigger problem here is the enormous ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The environmental impact of AI models has seen...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How did the advancements in GPT-4 and the subs...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nThing...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The advancements in GPT-4 and subsequent devel...</td>\n",
              "      <td>The advancements in GPT-4 and subsequent techn...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How have advancements in energy efficiency and...</td>\n",
              "      <td>[The much bigger problem here is the enormous ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Advancements in energy efficiency and the envi...</td>\n",
              "      <td>Advancements in energy efficiency and the envi...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How has the environmental impact of AI models ...</td>\n",
              "      <td>[The much bigger problem here is the enormous ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What were some key advancements in large langu...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison‚Äôs Weblog Subscribe ...</td>\n",
              "      <td>In 2023, there were several key advancements i...</td>\n",
              "      <td>In 2023, large language models (LLMs) saw sign...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How has Google's Gemini model contributed to t...</td>\n",
              "      <td>[Multimodal vision is common, audio and video ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\ngets you OpenAI‚Äôs most expensive m...</td>\n",
              "      <td>Google's Gemini model has significantly contri...</td>\n",
              "      <td>Google's Gemini model, particularly the Gemini...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How has the development of Claude 3 and its su...</td>\n",
              "      <td>[With Artifacts, Claude can write you an on-de...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The development of Claude 3 and its subsequent...</td>\n",
              "      <td>The development of Claude 3 and its subsequent...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How has Google's Gemini 1.5 Pro contributed to...</td>\n",
              "      <td>[I wrote about this at the time in The killer ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Google's Gemini 1.5 Pro has significantly cont...</td>\n",
              "      <td>Google's Gemini 1.5 Pro, released in February,...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0      Wut iz Meta's role in the development of LLMs?   \n",
              "1     What happen in September with prompt injection?   \n",
              "2                               Who is Simon Willson?   \n",
              "3   What insights has Simon Willison shared about ...   \n",
              "4   How has the environmental impact of AI models ...   \n",
              "5   How did the advancements in GPT-4 and the subs...   \n",
              "6   How have advancements in energy efficiency and...   \n",
              "7   How has the environmental impact of AI models ...   \n",
              "8   What were some key advancements in large langu...   \n",
              "9   How has Google's Gemini model contributed to t...   \n",
              "10  How has the development of Claude 3 and its su...   \n",
              "11  How has Google's Gemini 1.5 Pro contributed to...   \n",
              "\n",
              "                                   retrieved_contexts  \\\n",
              "0   [I wrote about how Large language models are h...   \n",
              "1   [But on the other hand, the things you sometim...   \n",
              "2   [Posted \\n\\n31st December 2023 at 11:59 pm ¬∑ F...   \n",
              "3   [Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...   \n",
              "4   [The much bigger problem here is the enormous ...   \n",
              "5   [Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nThing...   \n",
              "6   [The much bigger problem here is the enormous ...   \n",
              "7   [The much bigger problem here is the enormous ...   \n",
              "8   [Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...   \n",
              "9   [Multimodal vision is common, audio and video ...   \n",
              "10  [With Artifacts, Claude can write you an on-de...   \n",
              "11  [I wrote about this at the time in The killer ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison‚Äôs Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "5   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "6   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "7   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "8   [<1-hop>\\n\\nSimon Willison‚Äôs Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\ngets you OpenAI‚Äôs most expensive m...   \n",
              "10  [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "11  [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "\n",
              "                                             response  \\\n",
              "0   Meta's role in the development of Large Langua...   \n",
              "1   In September of the previous year, the term \"p...   \n",
              "2   Simon Willison is a writer and blogger who foc...   \n",
              "3   Simon Willison has shared several insights reg...   \n",
              "4   The environmental impact of AI models has seen...   \n",
              "5   The advancements in GPT-4 and subsequent devel...   \n",
              "6   Advancements in energy efficiency and the envi...   \n",
              "7   The environmental impact of AI models has impr...   \n",
              "8   In 2023, there were several key advancements i...   \n",
              "9   Google's Gemini model has significantly contri...   \n",
              "10  The development of Claude 3 and its subsequent...   \n",
              "11  Google's Gemini 1.5 Pro has significantly cont...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Meta has contributed to the development of LLM...   \n",
              "1   In September last year, the term 'prompt injec...   \n",
              "2   Simon Willison is the author of a weblog that ...   \n",
              "3   Simon Willison has discussed the profound impa...   \n",
              "4   The environmental impact of AI models has impr...   \n",
              "5   The advancements in GPT-4 and subsequent techn...   \n",
              "6   Advancements in energy efficiency and the envi...   \n",
              "7   The environmental impact of AI models has impr...   \n",
              "8   In 2023, large language models (LLMs) saw sign...   \n",
              "9   Google's Gemini model, particularly the Gemini...   \n",
              "10  The development of Claude 3 and its subsequent...   \n",
              "11  Google's Gemini 1.5 Pro, released in February,...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 288,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wut iz Meta's role in the development of LLMs?</td>\n",
              "      <td>[I wrote about how Large language models are h...</td>\n",
              "      <td>[Code may be the best application The ethics o...</td>\n",
              "      <td>Meta's role in the development of Large Langua...</td>\n",
              "      <td>Meta has contributed to the development of LLM...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What happen in September with prompt injection?</td>\n",
              "      <td>[But on the other hand, the things you sometim...</td>\n",
              "      <td>[Based Development As a computer scientist and...</td>\n",
              "      <td>In September of the previous year, the term \"p...</td>\n",
              "      <td>In September last year, the term 'prompt injec...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who is Simon Willson?</td>\n",
              "      <td>[Posted \\n\\n31st December 2023 at 11:59 pm ¬∑ F...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison is a writer and blogger who foc...</td>\n",
              "      <td>Simon Willison is the author of a weblog that ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What insights has Simon Willison shared about ...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...</td>\n",
              "      <td>[easy to follow. The rest of the document incl...</td>\n",
              "      <td>Simon Willison has shared several insights reg...</td>\n",
              "      <td>Simon Willison has discussed the profound impa...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How has the environmental impact of AI models ...</td>\n",
              "      <td>[The much bigger problem here is the enormous ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The environmental impact of AI models has seen...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How did the advancements in GPT-4 and the subs...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nThing...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The advancements in GPT-4 and subsequent devel...</td>\n",
              "      <td>The advancements in GPT-4 and subsequent techn...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How have advancements in energy efficiency and...</td>\n",
              "      <td>[The much bigger problem here is the enormous ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Advancements in energy efficiency and the envi...</td>\n",
              "      <td>Advancements in energy efficiency and the envi...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How has the environmental impact of AI models ...</td>\n",
              "      <td>[The much bigger problem here is the enormous ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>The environmental impact of AI models has impr...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What were some key advancements in large langu...</td>\n",
              "      <td>[Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison‚Äôs Weblog Subscribe ...</td>\n",
              "      <td>In 2023, there were several key advancements i...</td>\n",
              "      <td>In 2023, large language models (LLMs) saw sign...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How has Google's Gemini model contributed to t...</td>\n",
              "      <td>[Multimodal vision is common, audio and video ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\ngets you OpenAI‚Äôs most expensive m...</td>\n",
              "      <td>Google's Gemini model has significantly contri...</td>\n",
              "      <td>Google's Gemini model, particularly the Gemini...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How has the development of Claude 3 and its su...</td>\n",
              "      <td>[With Artifacts, Claude can write you an on-de...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>The development of Claude 3 and its subsequent...</td>\n",
              "      <td>The development of Claude 3 and its subsequent...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How has Google's Gemini 1.5 Pro contributed to...</td>\n",
              "      <td>[I wrote about this at the time in The killer ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nPrompt driven app generation is a ...</td>\n",
              "      <td>Google's Gemini 1.5 Pro has significantly cont...</td>\n",
              "      <td>Google's Gemini 1.5 Pro, released in February,...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0      Wut iz Meta's role in the development of LLMs?   \n",
              "1     What happen in September with prompt injection?   \n",
              "2                               Who is Simon Willson?   \n",
              "3   What insights has Simon Willison shared about ...   \n",
              "4   How has the environmental impact of AI models ...   \n",
              "5   How did the advancements in GPT-4 and the subs...   \n",
              "6   How have advancements in energy efficiency and...   \n",
              "7   How has the environmental impact of AI models ...   \n",
              "8   What were some key advancements in large langu...   \n",
              "9   How has Google's Gemini model contributed to t...   \n",
              "10  How has the development of Claude 3 and its su...   \n",
              "11  How has Google's Gemini 1.5 Pro contributed to...   \n",
              "\n",
              "                                   retrieved_contexts  \\\n",
              "0   [I wrote about how Large language models are h...   \n",
              "1   [But on the other hand, the things you sometim...   \n",
              "2   [Posted \\n\\n31st December 2023 at 11:59 pm ¬∑ F...   \n",
              "3   [Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...   \n",
              "4   [The much bigger problem here is the enormous ...   \n",
              "5   [Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nThing...   \n",
              "6   [The much bigger problem here is the enormous ...   \n",
              "7   [The much bigger problem here is the enormous ...   \n",
              "8   [Simon Willison‚Äôs Weblog\\n\\nSubscribe\\n\\nStuff...   \n",
              "9   [Multimodal vision is common, audio and video ...   \n",
              "10  [With Artifacts, Claude can write you an on-de...   \n",
              "11  [I wrote about this at the time in The killer ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Code may be the best application The ethics o...   \n",
              "1   [Based Development As a computer scientist and...   \n",
              "2   [Simon Willison‚Äôs Weblog Subscribe Stuff we fi...   \n",
              "3   [easy to follow. The rest of the document incl...   \n",
              "4   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "5   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "6   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "7   [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "8   [<1-hop>\\n\\nSimon Willison‚Äôs Weblog Subscribe ...   \n",
              "9   [<1-hop>\\n\\ngets you OpenAI‚Äôs most expensive m...   \n",
              "10  [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "11  [<1-hop>\\n\\nPrompt driven app generation is a ...   \n",
              "\n",
              "                                             response  \\\n",
              "0   Meta's role in the development of Large Langua...   \n",
              "1   In September of the previous year, the term \"p...   \n",
              "2   Simon Willison is a writer and blogger who foc...   \n",
              "3   Simon Willison has shared several insights reg...   \n",
              "4   The environmental impact of AI models has seen...   \n",
              "5   The advancements in GPT-4 and subsequent devel...   \n",
              "6   Advancements in energy efficiency and the envi...   \n",
              "7   The environmental impact of AI models has impr...   \n",
              "8   In 2023, there were several key advancements i...   \n",
              "9   Google's Gemini model has significantly contri...   \n",
              "10  The development of Claude 3 and its subsequent...   \n",
              "11  Google's Gemini 1.5 Pro has significantly cont...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Meta has contributed to the development of LLM...   \n",
              "1   In September last year, the term 'prompt injec...   \n",
              "2   Simon Willison is the author of a weblog that ...   \n",
              "3   Simon Willison has discussed the profound impa...   \n",
              "4   The environmental impact of AI models has impr...   \n",
              "5   The advancements in GPT-4 and subsequent techn...   \n",
              "6   Advancements in energy efficiency and the envi...   \n",
              "7   The environmental impact of AI models has impr...   \n",
              "8   In 2023, large language models (LLMs) saw sign...   \n",
              "9   Google's Gemini model, particularly the Gemini...   \n",
              "10  The development of Claude 3 and its subsequent...   \n",
              "11  Google's Gemini 1.5 Pro, released in February,...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 289,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ft_dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "base_evaluation_dataset = EvaluationDataset.from_pandas(base_dataset.to_pandas())\n",
        "ft_evaluation_dataset = EvaluationDataset.from_pandas(ft_dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98a39e8ce92c4d7e8720f564e4264716",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d2f7ff3a276427fabdab49afbdb9715",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[41]: TimeoutError()\n"
          ]
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "base_result = evaluate(\n",
        "    dataset=base_evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "\n",
        "ft_result = evaluate(\n",
        "    dataset=ft_evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.7816, 'faithfulness': 0.9268, 'factual_correctness': 0.6267, 'answer_relevancy': 0.7774, 'context_entity_recall': 0.4477, 'noise_sensitivity_relevant': 0.2554}"
            ]
          },
          "execution_count": 293,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.7955, 'faithfulness': 0.9335, 'factual_correctness': 0.6333, 'answer_relevancy': 0.8559, 'context_entity_recall': 0.4366, 'noise_sensitivity_relevant': 0.2491}"
            ]
          },
          "execution_count": 294,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ft_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis of Base and Fine-tuned Model Results\n",
        "\n",
        "Here‚Äôs a detailed analysis comparing the `base_result` and `ft_result` dictionaries, including percentage changes for each metric.\n",
        "\n",
        "## 1. Context Recall\n",
        "- **Base Result:** 0.7816\n",
        "- **Fine-tuned Result:** 0.7955\n",
        "- **Percentage Change:** +1.79%\n",
        "- **Analysis:** The fine-tuned model shows a slight improvement in context recall, indicating that it is better at retrieving relevant contexts related to the questions posed. This suggests that the fine-tuning process has enhanced the model's ability to understand and recall relevant information from the context.\n",
        "\n",
        "## 2. Faithfulness\n",
        "- **Base Result:** 0.9268\n",
        "- **Fine-tuned Result:** 0.9335\n",
        "- **Percentage Change:** +0.76%\n",
        "- **Analysis:** The fine-tuned model also demonstrates a higher faithfulness score, which means it is more likely to provide answers that are consistent with the provided context. This improvement indicates that the fine-tuning has helped the model maintain accuracy in its responses, ensuring that the information it provides aligns closely with the context.\n",
        "\n",
        "## 3. Factual Correctness\n",
        "- **Base Result:** 0.6267\n",
        "- **Fine-tuned Result:** 0.6333\n",
        "- **Percentage Change:** +1.05%\n",
        "- **Analysis:** There is a marginal increase in factual correctness in the fine-tuned model. While both models have relatively low scores in this area, the fine-tuned model is slightly better at providing factually accurate information. This suggests that the fine-tuning process has had a positive impact on the model's ability to deliver correct information.\n",
        "\n",
        "## 4. Answer Relevancy\n",
        "- **Base Result:** 0.7774\n",
        "- **Fine-tuned Result:** 0.8559\n",
        "- **Percentage Change:** +10.06%\n",
        "- **Analysis:** The fine-tuned model shows a significant improvement in answer relevancy. This indicates that the responses generated by the fine-tuned model are more pertinent to the questions asked, suggesting that the fine-tuning has effectively enhanced the model's ability to generate contextually appropriate answers.\n",
        "\n",
        "## 5. Context Entity Recall\n",
        "- **Base Result:** 0.4477\n",
        "- **Fine-tuned Result:** 0.4366\n",
        "- **Percentage Change:** -2.48%\n",
        "- **Analysis:** Interestingly, the fine-tuned model shows a slight decrease in context entity recall. This metric measures the model's ability to identify and recall specific entities within the context. The drop may suggest that while the fine-tuned model is better at generating relevant answers, it may not be as effective at recognizing specific entities within the context.\n",
        "\n",
        "## 6. Noise Sensitivity Relevant\n",
        "- **Base Result:** 0.2554\n",
        "- **Fine-tuned Result:** 0.2491\n",
        "- **Percentage Change:** -2.47%\n",
        "- **Analysis:** The fine-tuned model has a lower score in noise sensitivity relevant, indicating that it may be slightly less affected by irrelevant or noisy information in the context. This could imply that the fine-tuning has helped the model focus more on relevant information, although the overall scores in this area are low for both models.\n",
        "\n",
        "## Summary\n",
        "Overall, the fine-tuned model demonstrates improvements in most metrics, particularly in context recall, faithfulness, and answer relevancy, with percentage changes of +1.79%, +0.76%, and +10.06%, respectively. These enhancements suggest that the fine-tuning process has effectively improved the model's ability to generate accurate and relevant responses based on the provided context. However, the slight decreases in context entity recall and noise sensitivity relevant, with percentage changes of -2.48% and -2.47%, indicate areas where the model may need further refinement.\n",
        "\n",
        "In conclusion, the fine-tuned model appears to be a better performer overall, especially in generating relevant and contextually accurate answers, which is crucial for applications requiring high-quality information retrieval and response generation."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006bf6f5ebaf400486a6b82610381db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d3fc6edfdab4fe9aff8e805632eadad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebe8aa7a82124b57bce2d826c1dea0fa",
              "IPY_MODEL_aa11c10b4234452d9430744ab89b59a2",
              "IPY_MODEL_ca05cbcd72cb41d9856804ffb17b26cb"
            ],
            "layout": "IPY_MODEL_cc2a33e9a7ac4c5699346fcbf53b7c95"
          }
        },
        "1016780729b04ba489d488922173ddae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9198dd0fa8f04aa7b75307aa2d513bfb",
            "style": "IPY_MODEL_59cd26ae53024fbd85431b6683cd119c",
            "value": true
          }
        },
        "13e9bf583f6442d395334947379a280a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18825dc83221412ab602830fc00db71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ea48a80c194d128959422368aa0e10",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9f4026c62c60493caa18c014ae414e65",
            "value": "Connecting..."
          }
        },
        "1995b4d98fe044e38f1980d47033ca40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a872283afaa4a33a9bc3f9e57b3650b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381c780ce17e4662981175400fb8a0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "408f4dfce21a45cfad36047677ec8658": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a723c6a56e94309b2e3abe63f28aedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869814ecd49e46f9a33dd53130e6953f",
            "max": 1336413848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc7d460d3c5a4ac6a9b64929736d6598",
            "value": 1336413848
          }
        },
        "4e2c257232c3472697e03350de43cb30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7ccd97042c4fb9a201b6dc76762e04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58699484312f460cb96ac44e3af14aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_d4acc9a7aca04564bfaefe33de079394"
          }
        },
        "59cd26ae53024fbd85431b6683cd119c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72ec09e2cbbf4788b1561abfcdd0819a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "869814ecd49e46f9a33dd53130e6953f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878dc96f87dd45f389948a546db33e94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9198dd0fa8f04aa7b75307aa2d513bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95160a05de5b402b9bdb0bd2d099cd00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9638a0456e0c41b1b9b9757932f55c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d34dac405fd40139d5dc04eecef55ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4e7ccd97042c4fb9a201b6dc76762e04",
            "style": "IPY_MODEL_72ec09e2cbbf4788b1561abfcdd0819a",
            "tooltip": ""
          }
        },
        "9f4026c62c60493caa18c014ae414e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ea48a80c194d128959422368aa0e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa11c10b4234452d9430744ab89b59a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a872283afaa4a33a9bc3f9e57b3650b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe4d1052824c4ed29c38c5311087e650",
            "value": 1
          }
        },
        "acf3991e5d644f468264f561b28b514a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95160a05de5b402b9bdb0bd2d099cd00",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf376b0ea3544055b8867d7013526502",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "b011a6ccf8e745c6be6f3a17b7d61dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_878dc96f87dd45f389948a546db33e94",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_006bf6f5ebaf400486a6b82610381db0",
            "value": ""
          }
        },
        "bfc4997e3bd94e66bebaa1ffdae1b99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca05cbcd72cb41d9856804ffb17b26cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e283b1608c4d4266a03c57dc95aabb2e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bfc4997e3bd94e66bebaa1ffdae1b99e",
            "value": "‚Äá0/1‚Äá[00:00&lt;?,‚Äá?example/s]"
          }
        },
        "ca5804644ef345c1b4f670fb7f088fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1f0bc4a1a74568b9c74a7392a1568f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2a33e9a7ac4c5699346fcbf53b7c95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "cc7d460d3c5a4ac6a9b64929736d6598": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf376b0ea3544055b8867d7013526502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d45a7d95e5b14a6f88d5798fec9c40a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e2c257232c3472697e03350de43cb30",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_381c780ce17e4662981175400fb8a0f8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d4acc9a7aca04564bfaefe33de079394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d6169577ffb341a69eb0175e301b6a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1e19624fde4d3c8048ce00264d6056",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9638a0456e0c41b1b9b9757932f55c53",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e283b1608c4d4266a03c57dc95aabb2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c86e0e33264ed8b6325e44f9421653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acf3991e5d644f468264f561b28b514a",
              "IPY_MODEL_4a723c6a56e94309b2e3abe63f28aedc",
              "IPY_MODEL_ef25768d3b0746b48c6d02e9bd151bf9"
            ],
            "layout": "IPY_MODEL_cc1f0bc4a1a74568b9c74a7392a1568f"
          }
        },
        "ebe8aa7a82124b57bce2d826c1dea0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408f4dfce21a45cfad36047677ec8658",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ca5804644ef345c1b4f670fb7f088fe8",
            "value": "Computing‚Äáwidget‚Äáexamples:‚Äá‚Äá‚Äá0%"
          }
        },
        "ef25768d3b0746b48c6d02e9bd151bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e9bf583f6442d395334947379a280a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1995b4d98fe044e38f1980d47033ca40",
            "value": "‚Äá1.34G/1.34G‚Äá[01:11&lt;00:00,‚Äá21.0MB/s]"
          }
        },
        "fb1e19624fde4d3c8048ce00264d6056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4d1052824c4ed29c38c5311087e650": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
