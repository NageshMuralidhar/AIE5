{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank)).\n",
        "\n",
        "> You do not need to run the following cells if you are running this notebook locally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/233.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.1/233.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.1/378.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -qU langchain langchain-openai langchain-cohere rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0pDRFEWSXvh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-02 13:16:04--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19628 (19K) [text/plain]\n",
            "Saving to: ‘john_wick_1.csv’\n",
            "\n",
            "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-02 13:16:04 (1.15 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
            "\n",
            "--2025-03-02 13:16:04--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14747 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_2.csv’\n",
            "\n",
            "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-03-02 13:16:05 (1.35 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
            "\n",
            "--2025-03-02 13:16:05--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13888 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_3.csv’\n",
            "\n",
            "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-03-02 13:16:05 (934 KB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
            "\n",
            "--2025-03-02 13:16:05--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15109 (15K) [text/plain]\n",
            "Saving to: ‘john_wick_4.csv’\n",
            "\n",
            "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-03-02 13:16:05 (1.36 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2025, 2, 27, 13, 16, 20, 268454)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the positive reviews provided.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review:\\n\\n- [Review by ymyuseda](/review/rw4854296/?ref_=tt_urv)'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement seeking vengeance after the gangsters kill his dog and take everything from him. To exact his revenge, he unleashes a maelstrom of destruction against those who cross him, leading to intense action and thrilling fights.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Opinions about John Wick seem to vary. Some people really enjoyed the action and style of the movie, while others found it lacking in plot and substance. It seems like it's a matter of personal preference.\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, the main character, played by Keanu Reeves, experiences emotional setup and beautifully choreographed action sequences. It is a highly recommended movie, especially for those who enjoy action films.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided in the context.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: /review/rw4854296/?ref_=tt_urv.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, the main character, John Wick, is forced back into the world of crime when a mobster asks him to carry out a hit. When he completes the task, a contract is put on him, leading to chaos and intense action.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems that people generally liked John Wick. The reviews praised the action sequences, Keanu Reeves' performance, and the overall entertainment value of the movie.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I\\'m sorry, there are no reviews with a rating of 10 for the movie \"John Wick 4.\"'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick: Chapter 2, former hitman John Wick is forced back into action when an Italian crime lord calls in a favor. Wick must carry out an assignment to kill the crime lord's sister in Rome in order to fulfill this favor. However, after completing the task, the crime lord puts a bounty on Wick's head, leading to a series of intense action sequences as Wick is hunted by professional killers. The movie is filled with action, suspense, and thrilling fight scenes.\""
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_44187/3574430551.py:8: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the reviews provided, opinions on John Wick seem to be divided. Some people really enjoy the movie and find it to be a wild ride, while others are critical of its plot and fight scenes.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". The URL to that review is: /review/rw4854296/?ref_=tt_urv'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, a retired assassin named John Wick comes out of retirement when someone kills his dog. In the sequel, John Wick 2, he is forced back into the world of assassins to pay off an old debt and ends up killing many assassins in Italy, Canada, and Manhattan.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People generally liked John Wick based on the positive reviews given by critics.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review for \"John Wick 3\" with a rating of 10. Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\''"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to seek revenge on the gangsters that killed his dog and took everything from him, ultimately leading to a series of violent and action-packed confrontations as he faces off against numerous enemies seeking to take him down.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!\n",
        "\n",
        "> NOTE: You do not need to run this cell if you're running this locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Overall, the reviews for John Wick were mostly positive with many people enjoying it.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\''"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, the protagonist, John Wick, seeks revenge on the people who took something he loved from him. Initially, it was his dog that was killed, leading him to unleash his lethal capacity against gangsters who also stole his car. The movie focuses on action, stylish stunts, kinetic chaos, and a relatable hero seeking vengeance.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activity #1 Begins Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0a8eaacec274f9dbcb53de4291f8c7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7e931cbcb604c43ae056a4063623e15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node cd903521-f20b-4076-bad0-23e12f3cffba does not have a summary. Skipping filtering.\n",
            "Node 82e643a2-92f3-4af0-bac0-c3b62eae480d does not have a summary. Skipping filtering.\n",
            "Node 17e0c9bf-2e14-4fb2-8c3a-9b600ba5af7b does not have a summary. Skipping filtering.\n",
            "Node d1306e41-94ae-4ad7-95b8-4e76c4899815 does not have a summary. Skipping filtering.\n",
            "Node 0b75e5aa-dc65-4ed3-bb1f-c8d7e67f619e does not have a summary. Skipping filtering.\n",
            "Node c61b00ff-9b42-44e9-9150-9597a9e577bd does not have a summary. Skipping filtering.\n",
            "Node 93cbf8a9-947a-44ea-ac81-1901a21086e3 does not have a summary. Skipping filtering.\n",
            "Node 7877a68b-4bcc-4050-bf5c-4b68e97c60b8 does not have a summary. Skipping filtering.\n",
            "Node ce88d054-bded-4d91-abfb-82880aec1f38 does not have a summary. Skipping filtering.\n",
            "Node 4245bbae-b829-4dcc-aed9-27cf6de2b57d does not have a summary. Skipping filtering.\n",
            "Node ca188f73-272f-4f4d-b794-0d535b6cd53f does not have a summary. Skipping filtering.\n",
            "Node 86361dd0-00db-4bca-8677-d05eda2ed53d does not have a summary. Skipping filtering.\n",
            "Node 5a52702f-e061-4a87-a963-014b240c089c does not have a summary. Skipping filtering.\n",
            "Node ca5bbce0-5029-4a31-9a6b-1914ddfe8312 does not have a summary. Skipping filtering.\n",
            "Node 5c4268d5-6d39-4851-8a28-54340b8a327b does not have a summary. Skipping filtering.\n",
            "Node ec641c9f-d280-48a7-805c-5a9433287d9b does not have a summary. Skipping filtering.\n",
            "Node 4472bfce-5acd-470c-ab66-fed4ebd9ea88 does not have a summary. Skipping filtering.\n",
            "Node 658f2e67-6356-4180-b2a7-11ebbaf7a03f does not have a summary. Skipping filtering.\n",
            "Node 7db9229b-80ef-4ff7-b417-8cf17cc639e7 does not have a summary. Skipping filtering.\n",
            "Node 142d7ff0-02fd-4f3a-b1ab-9ca91548cb56 does not have a summary. Skipping filtering.\n",
            "Node 92544aac-aca7-4b66-a9d4-52a65551ccd3 does not have a summary. Skipping filtering.\n",
            "Node 704691fe-b671-404c-9905-2752582c6045 does not have a summary. Skipping filtering.\n",
            "Node ab27521c-9925-4505-a892-d8e8cd39fb5e does not have a summary. Skipping filtering.\n",
            "Node 3473c0aa-58a4-467e-b73b-61e5904f8581 does not have a summary. Skipping filtering.\n",
            "Node 02519e17-dad7-4c4c-b2d6-8cf30f905c59 does not have a summary. Skipping filtering.\n",
            "Node 7c016bfb-50e6-4c10-9d11-890b5e1ffeb8 does not have a summary. Skipping filtering.\n",
            "Node 07bb244d-b3f3-4209-a8c9-d454a860f3c2 does not have a summary. Skipping filtering.\n",
            "Node 16b32a29-c1a4-4524-a3d5-a2e195abae02 does not have a summary. Skipping filtering.\n",
            "Node 9082417c-db24-4ea6-83a2-1fc7184afe66 does not have a summary. Skipping filtering.\n",
            "Node 5120ae0c-572e-4103-9e71-9a1441e48242 does not have a summary. Skipping filtering.\n",
            "Node 700fc7aa-d4b1-44f8-9328-c5127e63a4aa does not have a summary. Skipping filtering.\n",
            "Node bc07c91c-654a-4887-822d-b3438cf1d04e does not have a summary. Skipping filtering.\n",
            "Node f5d604cf-5043-4577-b9c3-77c7b78e2774 does not have a summary. Skipping filtering.\n",
            "Node ff24c6ad-7674-41b1-9216-c11bbfd9dd73 does not have a summary. Skipping filtering.\n",
            "Node 36027c94-439c-4312-98e3-06226f095506 does not have a summary. Skipping filtering.\n",
            "Node b22aa1c7-201e-4677-b33f-8b3df55b2dae does not have a summary. Skipping filtering.\n",
            "Node 510469d9-15fd-4589-bd65-76f93d391179 does not have a summary. Skipping filtering.\n",
            "Node 41974832-37f7-4cf6-86ae-f0cff36d89fe does not have a summary. Skipping filtering.\n",
            "Node 58ae1f1b-a267-4516-acf6-a947ef71db8f does not have a summary. Skipping filtering.\n",
            "Node b8c7dd49-4015-47b5-abfe-08e8dcf2c90c does not have a summary. Skipping filtering.\n",
            "Node e4e1315d-4c09-4368-b633-608170d1737b does not have a summary. Skipping filtering.\n",
            "Node 20d87c99-ec86-4787-a81b-e59a7b7ee1ab does not have a summary. Skipping filtering.\n",
            "Node 1f7da14e-cd50-4012-abc2-6ed1048507b4 does not have a summary. Skipping filtering.\n",
            "Node 59606330-2955-4fde-a188-bd64d1f2d208 does not have a summary. Skipping filtering.\n",
            "Node af9521f3-ef9d-44d8-9243-eebff724d5e5 does not have a summary. Skipping filtering.\n",
            "Node be6f8e32-853f-4d9f-b38d-a9f8be4fee2e does not have a summary. Skipping filtering.\n",
            "Node fa73addc-8412-482c-a4fe-5c2c4f2bdf01 does not have a summary. Skipping filtering.\n",
            "Node 056257f7-9bf4-4eb9-9383-5668bf24ca2f does not have a summary. Skipping filtering.\n",
            "Node 0a42ff91-18a5-443e-9774-e8090416a766 does not have a summary. Skipping filtering.\n",
            "Node 6b8b6c6c-ed1d-453c-b0c2-ea73d3e1e70b does not have a summary. Skipping filtering.\n",
            "Node d022a241-2eba-4c56-949c-c76eef64151f does not have a summary. Skipping filtering.\n",
            "Node 27249233-af8a-4593-af30-a85383a2b77d does not have a summary. Skipping filtering.\n",
            "Node aefc7182-c895-40f4-9073-e473270bbaaf does not have a summary. Skipping filtering.\n",
            "Node ca20ec96-8acd-40d9-9f98-621fc22278de does not have a summary. Skipping filtering.\n",
            "Node f8b2aed5-a411-4054-b653-b55df55387c6 does not have a summary. Skipping filtering.\n",
            "Node 381a8db8-4c79-4de1-890c-05959186b00a does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8da5c72cea6e466fa2d55f3bf75a7c02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/244 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16a9a01025cb4fca94605d7b5c09ae4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36d9b50538d443a8b0676cb1c462c3ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67de2bed962245a89eab1029b911807b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5aaefcd8e3a4fa4a6536f74815e030a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(documents, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  John Wick's premise is beautifully simple, foc...   \n",
              "1  John Wick is a film series that has gained imm...   \n",
              "2  Keanu's performance in John Wick stands out du...   \n",
              "3  John Wick, a retired assassin known as the \"Bo...   \n",
              "4  In John Wick, the Russian mob plays a signific...   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...   \n",
              "6  John Wick 3 is described as the best action mo...   \n",
              "7  John Wick 3 is noted for creating something sp...   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...   \n",
              "9  The latest John Wick film was a bitter disappo...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"RAGAS_APP_TOKEN\"] = getpass(\"Please enter your Ragas API key!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Naive Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = naive_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e260d471e694b169abda04b732a6b5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9667, 'context_entity_recall': 0.2643, 'noise_sensitivity_relevant': 0.4913, 'context_precision': 0.8654}"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The premise of John Wick, which is a simple ye...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>John Wick has become a beloved franchise becau...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.976543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 18\\nReview: When the story begins, John (Ke...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie, \"John Wick,\" John Wick, played b...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.778333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[: 18\\nReview: When the story begins, John (Ke...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>The Russian mob plays a significant role in th...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.841667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick's character development in Chapter 3...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[: 1\\nReview: I'm a fan of the John Wick films...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>I'm sorry, but I don't have specific details c...</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.749660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>John Wick 3 distinguishes itself from John Wic...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.814286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[: 13\\nReview: Following on from two delirious...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum is praised f...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.885714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[: 17\\nReview: There are actually quite a hand...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>I'm sorry, I do not have specific information ...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.941518</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "3  [: 18\\nReview: When the story begins, John (Ke...   \n",
              "4  [: 18\\nReview: When the story begins, John (Ke...   \n",
              "5  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "6  [: 1\\nReview: I'm a fan of the John Wick films...   \n",
              "7  [: 22\\nReview: Lets contemplate about componen...   \n",
              "8  [: 13\\nReview: Following on from two delirious...   \n",
              "9  [: 17\\nReview: There are actually quite a hand...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The premise of John Wick, which is a simple ye...   \n",
              "1  John Wick has become a beloved franchise becau...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie, \"John Wick,\" John Wick, played b...   \n",
              "4  The Russian mob plays a significant role in th...   \n",
              "5  John Wick's character development in Chapter 3...   \n",
              "6  I'm sorry, but I don't have specific details c...   \n",
              "7  John Wick 3 distinguishes itself from John Wic...   \n",
              "8  John Wick: Chapter 3 - Parabellum is praised f...   \n",
              "9  I'm sorry, I do not have specific information ...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  John Wick's premise is beautifully simple, foc...        1.000000   \n",
              "1  John Wick is a film series that has gained imm...        1.000000   \n",
              "2  Keanu's performance in John Wick stands out du...        1.000000   \n",
              "3  John Wick, a retired assassin known as the \"Bo...        1.000000   \n",
              "4  In John Wick, the Russian mob plays a signific...        1.000000   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...        0.666667   \n",
              "6  John Wick 3 is described as the best action mo...        1.000000   \n",
              "7  John Wick 3 is noted for creating something sp...        1.000000   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...        1.000000   \n",
              "9  The latest John Wick film was a bitter disappo...        1.000000   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.125000                    0.142857           1.000000  \n",
              "1               0.250000                    0.272727           0.976543  \n",
              "2               0.666667                    0.555556           1.000000  \n",
              "3               0.250000                    0.666667           0.778333  \n",
              "4               0.142857                    0.692308           0.841667  \n",
              "5               0.000000                    0.000000           0.666667  \n",
              "6               0.000000                    1.000000           0.749660  \n",
              "7               0.500000                    0.666667           0.814286  \n",
              "8               0.375000                    0.250000           0.885714  \n",
              "9               0.333333                    0.666667           0.941518  "
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric                      | Values                                                      |\n",
        "|-----------------------------|------------------------------------------------------------|\n",
        "| Retrieval method            | Naive Retrieval                                            |\n",
        "| context_recall              | [1.0, 1.0, 1.0, 0.6667, 0.5, 0.6667, 1.0, 1.0, 1.0, 1.0]    |\n",
        "| context_precision           | [0.9095, 0.5010, 0.8955, 0.8441, 0.9444, 0.9889, 1.0, 0.7282, 0.8304, 1.0] |\n",
        "| noise_sensitivity_relevant  | [0.6667, 0.0, 0.125, 0.25, 0.0, nan, 0.0, nan, 0.4, 0.1]    |\n",
        "| context_precision (repeat)   | [0.9095, 0.5010, 0.8955, 0.8441, 0.9444, 0.9889, 1.0, 0.7282, 0.8304, 1.0] |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BM25 Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = bm25_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11c1c91faed4479d999248bb7a5a10f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.8083, 'context_entity_recall': 0.2994, 'noise_sensitivity_relevant': 0.0950, 'context_precision': 0.6583}"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[: 11\\nReview: Who needs a 2hr and 40 min acti...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The premise of John Wick, where the protagonis...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>John Wick is loved by many for its slickness, ...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu's performance in John Wick stands out in...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 5\\nReview: What is all the raving about wit...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>I'm sorry, I don't know what happens to John W...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[: 14\\nReview: Another significant increase in...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>The Russian mob plays a significant role in th...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick's character development in Chapter 3...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[: 10\\nReview: The first John Wick film took m...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[: 23\\nReview: I love me a bit of the old ultr...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>The uniqueness of John Wick 3 compared to John...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[: 13\\nReview: Following on from two delirious...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>The key themes and elements that make John Wic...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[: 10\\nReview: The first John Wick film took m...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The latest John Wick film was a disappointment...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 11\\nReview: Who needs a 2hr and 40 min acti...   \n",
              "1  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 5\\nReview: What is all the raving about wit...   \n",
              "4  [: 14\\nReview: Another significant increase in...   \n",
              "5  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "6  [: 10\\nReview: The first John Wick film took m...   \n",
              "7  [: 23\\nReview: I love me a bit of the old ultr...   \n",
              "8  [: 13\\nReview: Following on from two delirious...   \n",
              "9  [: 10\\nReview: The first John Wick film took m...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The premise of John Wick, where the protagonis...   \n",
              "1  John Wick is loved by many for its slickness, ...   \n",
              "2  Keanu's performance in John Wick stands out in...   \n",
              "3  I'm sorry, I don't know what happens to John W...   \n",
              "4  The Russian mob plays a significant role in th...   \n",
              "5  John Wick's character development in Chapter 3...   \n",
              "6                                      I don't know.   \n",
              "7  The uniqueness of John Wick 3 compared to John...   \n",
              "8  The key themes and elements that make John Wic...   \n",
              "9  The latest John Wick film was a disappointment...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  John Wick's premise is beautifully simple, foc...        1.000000   \n",
              "1  John Wick is a film series that has gained imm...        1.000000   \n",
              "2  Keanu's performance in John Wick stands out du...        1.000000   \n",
              "3  John Wick, a retired assassin known as the \"Bo...        0.000000   \n",
              "4  In John Wick, the Russian mob plays a signific...        1.000000   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...        0.666667   \n",
              "6  John Wick 3 is described as the best action mo...        0.750000   \n",
              "7  John Wick 3 is noted for creating something sp...        1.000000   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...        1.000000   \n",
              "9  The latest John Wick film was a bitter disappo...        0.666667   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.125000                        0.00           0.500000  \n",
              "1               0.250000                        0.25           1.000000  \n",
              "2               0.666667                        0.20           0.750000  \n",
              "3               0.250000                        0.00           0.000000  \n",
              "4               0.285714                        0.00           0.583333  \n",
              "5               0.000000                        0.40           1.000000  \n",
              "6               0.250000                        0.00           0.500000  \n",
              "7               0.166667                        0.00           1.000000  \n",
              "8               0.000000                        0.10           0.916667  \n",
              "9               1.000000                        0.00           0.333333  "
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method: BM25 Retrieval\n",
            "context_recall [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 0.6666666666666666]\n",
            "context_precision [0.499999999975, 0.999999999975, 0.7499999999625, 0.0, 0.5833333333041666, 0.9999999999666667, 0.499999999975, 0.99999999995, 0.9166666666361111, 0.3333333333]\n",
            "noise_sensitivity_relevant [np.float64(0.0), np.float64(0.25), np.float64(0.2), np.float64(0.0), np.float64(0.0), np.float64(0.4), np.float64(0.0), np.float64(0.0), np.float64(0.1), np.float64(0.0)]\n",
            "context_precision [0.499999999975, 0.999999999975, 0.7499999999625, 0.0, 0.5833333333041666, 0.9999999999666667, 0.499999999975, 0.99999999995, 0.9166666666361111, 0.3333333333]\n"
          ]
        }
      ],
      "source": [
        "print('Retrieval method: BM25 Retrieval')\n",
        "print('context_recall', result['context_recall'])\n",
        "print('context_precision', result['context_precision'])\n",
        "print('noise_sensitivity_relevant', result['noise_sensitivity_relevant'])\n",
        "print('context_precision', result['context_precision'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric                      | Values                                                      |\n",
        "|-----------------------------|------------------------------------------------------------|\n",
        "| Retrieval method            | BM25 Retrieval                                             |\n",
        "| context_recall              | [0.5, 0.5, 1.0, 0.6667, 0.75, 0.6667, 1.0, 0.5, 0.5, 0.5]   |\n",
        "| context_precision           | [0.8333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333, 1.0]    |\n",
        "| noise_sensitivity_relevant  | [0.375, 0.0, 0.3333, 0.0, 0.0, 1.0, 0.0, 0.5, 0.1667, 0.3333] |\n",
        "| context_precision (repeat)   | [0.8333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333, 1.0]    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = contextual_compression_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55bffa2197974d0fa93f7b03b8de2753",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.6500, 'context_entity_recall': 0.2476, 'noise_sensitivity_relevant': 0.3823, 'context_precision': 0.9333}"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The premise of John Wick, which involves a man...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 18\\nReview: Ever since the original John Wi...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>John Wick has garnered significant love and at...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>John Wick is targeted by various professional ...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>The Russian mob plays a significant role in th...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick's character development in Chapter 3...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[: 19\\nReview: The inevitable third chapter of...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>Based on the reviews provided, the action qual...</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>In John Wick 3, the uniqueness compared to Joh...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>The key themes and elements that make John Wic...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[: 14\\nReview: By now you know what to expect ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The latest John Wick film was not a disappoint...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 18\\nReview: Ever since the original John Wi...   \n",
              "2  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "3  [: 20\\nReview: After resolving his issues with...   \n",
              "4  [: 20\\nReview: After resolving his issues with...   \n",
              "5  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "6  [: 19\\nReview: The inevitable third chapter of...   \n",
              "7  [: 22\\nReview: Lets contemplate about componen...   \n",
              "8  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "9  [: 14\\nReview: By now you know what to expect ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The premise of John Wick, which involves a man...   \n",
              "1  John Wick has garnered significant love and at...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  John Wick is targeted by various professional ...   \n",
              "4  The Russian mob plays a significant role in th...   \n",
              "5  John Wick's character development in Chapter 3...   \n",
              "6  Based on the reviews provided, the action qual...   \n",
              "7  In John Wick 3, the uniqueness compared to Joh...   \n",
              "8  The key themes and elements that make John Wic...   \n",
              "9  The latest John Wick film was not a disappoint...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  John Wick's premise is beautifully simple, foc...        1.000000   \n",
              "1  John Wick is a film series that has gained imm...        1.000000   \n",
              "2  Keanu's performance in John Wick stands out du...        1.000000   \n",
              "3  John Wick, a retired assassin known as the \"Bo...        0.000000   \n",
              "4  In John Wick, the Russian mob plays a signific...        0.666667   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...        0.666667   \n",
              "6  John Wick 3 is described as the best action mo...        0.000000   \n",
              "7  John Wick 3 is noted for creating something sp...        0.666667   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...        1.000000   \n",
              "9  The latest John Wick film was a bitter disappo...        0.500000   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.125000                    0.000000           1.000000  \n",
              "1               0.250000                    0.500000           1.000000  \n",
              "2               0.666667                    0.454545           1.000000  \n",
              "3               0.250000                    1.000000           0.333333  \n",
              "4               0.142857                    0.750000           1.000000  \n",
              "5               0.000000                    0.285714           1.000000  \n",
              "6               0.250000                    0.272727           1.000000  \n",
              "7               0.333333                    0.142857           1.000000  \n",
              "8               0.125000                    0.416667           1.000000  \n",
              "9               0.333333                    0.000000           1.000000  "
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method: Contextual Compression Retrieval\n",
            "context_recall [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.5]\n",
            "context_precision [0.9999999999666667, 0.9999999999666667, 0.99999999995, 0.3333333333, 0.9999999999666667, 0.9999999999, 0.99999999995, 0.99999999995, 0.9999999999666667, 0.9999999999666667]\n",
            "noise_sensitivity_relevant [np.float64(0.0), np.float64(0.5), np.float64(0.45454545454545453), np.float64(1.0), np.float64(0.75), np.float64(0.2857142857142857), np.float64(0.2727272727272727), np.float64(0.14285714285714285), np.float64(0.4166666666666667), np.float64(0.0)]\n",
            "context_precision [0.9999999999666667, 0.9999999999666667, 0.99999999995, 0.3333333333, 0.9999999999666667, 0.9999999999, 0.99999999995, 0.99999999995, 0.9999999999666667, 0.9999999999666667]\n"
          ]
        }
      ],
      "source": [
        "print('Retrieval method: Contextual Compression Retrieval')\n",
        "print('context_recall', result['context_recall'])\n",
        "print('context_precision', result['context_precision'])\n",
        "print('noise_sensitivity_relevant', result['noise_sensitivity_relevant'])\n",
        "print('context_precision', result['context_precision'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric                      | Values                                                      |\n",
        "|-----------------------------|------------------------------------------------------------|\n",
        "| Retrieval method            | Contextual Compression Retrieval                            |\n",
        "| context_recall              | [0.5, 0.5, 1.0, 0.6667, 0.75, 0.6667, 1.0, 0.5, 0.5, 0.5]   |\n",
        "| context_precision           | [0.8333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333, 1.0]    |\n",
        "| noise_sensitivity_relevant  | [0.75, 0.0, 0.3333, 0.0, 0.0, 1.0, 0.0, 0.5, 0.1667, 0.2222] |\n",
        "| context_precision (repeat)   | [0.8333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333, 1.0]    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-Query Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = multi_query_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c089e49477a4882b0654f47a0b3a60b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[34]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9167, 'context_entity_recall': 0.2823, 'noise_sensitivity_relevant': 0.5841, 'context_precision': 0.7933}"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The premise of John Wick, where the main chara...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.876816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 18\\nReview: Ever since the original John Wi...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>John Wick has gained immense popularity and lo...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.989744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.881346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 18\\nReview: When the story begins, John (Ke...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick,\" John experiences a p...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.859259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[: 18\\nReview: When the story begins, John (Ke...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>The Russian mob plays a significant role in th...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.961735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, the char...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.619048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>I am sorry, I do not have information comparin...</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.558895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>The elements that contribute to the uniqueness...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.640415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[: 13\\nReview: Following on from two delirious...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum is praised f...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.661088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[: 17\\nReview: There are actually quite a hand...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The latest John Wick film was considered a dis...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.884354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "1  [: 18\\nReview: Ever since the original John Wi...   \n",
              "2  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "3  [: 18\\nReview: When the story begins, John (Ke...   \n",
              "4  [: 18\\nReview: When the story begins, John (Ke...   \n",
              "5  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "6  [: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...   \n",
              "7  [: 3\\nReview: John wick has a very simple reve...   \n",
              "8  [: 13\\nReview: Following on from two delirious...   \n",
              "9  [: 17\\nReview: There are actually quite a hand...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The premise of John Wick, where the main chara...   \n",
              "1  John Wick has gained immense popularity and lo...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie \"John Wick,\" John experiences a p...   \n",
              "4  The Russian mob plays a significant role in th...   \n",
              "5  In John Wick: Chapter 3 - Parabellum, the char...   \n",
              "6  I am sorry, I do not have information comparin...   \n",
              "7  The elements that contribute to the uniqueness...   \n",
              "8  John Wick: Chapter 3 - Parabellum is praised f...   \n",
              "9  The latest John Wick film was considered a dis...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  John Wick's premise is beautifully simple, foc...        1.000000   \n",
              "1  John Wick is a film series that has gained imm...        1.000000   \n",
              "2  Keanu's performance in John Wick stands out du...        1.000000   \n",
              "3  John Wick, a retired assassin known as the \"Bo...        1.000000   \n",
              "4  In John Wick, the Russian mob plays a signific...        1.000000   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...        0.666667   \n",
              "6  John Wick 3 is described as the best action mo...        1.000000   \n",
              "7  John Wick 3 is noted for creating something sp...        1.000000   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...        1.000000   \n",
              "9  The latest John Wick film was a bitter disappo...        0.500000   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.125000                    0.428571           0.876816  \n",
              "1               0.250000                    0.875000           0.989744  \n",
              "2               0.666667                    0.428571           0.881346  \n",
              "3               0.500000                    0.625000           0.859259  \n",
              "4               0.142857                    0.500000           0.961735  \n",
              "5               0.000000                    0.000000           0.619048  \n",
              "6               0.250000                    1.000000           0.558895  \n",
              "7               0.000000                    0.600000           0.640415  \n",
              "8               0.555556                         NaN           0.661088  \n",
              "9               0.333333                    0.800000           0.884354  "
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method:  Multi-Query Retrieval\n",
            "context_recall [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5]\n",
            "context_precision [0.8768161131717784, 0.9897435897359763, 0.8813455988367854, 0.8592592592449383, 0.9617346938638119, 0.6190476190352381, 0.5588945776375914, 0.6404151404059917, 0.661087677746999, 0.8843537414839651]\n",
            "noise_sensitivity_relevant [np.float64(0.42857142857142855), np.float64(0.875), np.float64(0.42857142857142855), np.float64(0.625), np.float64(0.5), np.float64(0.0), np.float64(1.0), np.float64(0.6), nan, np.float64(0.8)]\n",
            "context_precision [0.8768161131717784, 0.9897435897359763, 0.8813455988367854, 0.8592592592449383, 0.9617346938638119, 0.6190476190352381, 0.5588945776375914, 0.6404151404059917, 0.661087677746999, 0.8843537414839651]\n"
          ]
        }
      ],
      "source": [
        "print('Retrieval method:  Multi-Query Retrieval')\n",
        "print('context_recall', result['context_recall'])\n",
        "print('context_precision', result['context_precision'])\n",
        "print('noise_sensitivity_relevant', result['noise_sensitivity_relevant'])\n",
        "print('context_precision', result['context_precision'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric                      | Values                                                      |\n",
        "|-----------------------------|------------------------------------------------------------|\n",
        "| Retrieval method            | Multi-Query Retrieval                            |\n",
        "| context_recall              | [1.0, 1.0, 1.0, 0.6667, 1.0, 0.6667, 1.0, 1.0, 1.0, 1.0]    |\n",
        "| context_precision           | [1.0, 0.4292, 0.9583, 0.8632, 0.8898, 0.9172, 1.0, 0.5875, 0.8167, 0.8934] |\n",
        "| noise_sensitivity_relevant  | [0.6522, 0.0, 0.4, 0.2857, 0.5, 1.0, 0.0, 0.25, 0.8, 0.0909] |\n",
        "| context_precision (repeat)   | [1.0, 0.4292, 0.9583, 0.8632, 0.8898, 0.9172, 1.0, 0.5875, 0.8167, 0.8934] |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Document Retrieval\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = parent_document_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ddb9cc37ad046eb8401c762e69cf5d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.4250, 'context_entity_recall': 0.2300, 'noise_sensitivity_relevant': 0.3475, 'context_precision': 0.9417}"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[: 18\\nReview: Ever since the original John Wi...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The simple premise of John Wick, where the pro...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>John Wick is loved by many because it features...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[: 23\\nReview: Rating 10/10\\nI was able to cat...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick is high...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick 2\", John Wick is calle...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the plot of John Wick, the Russian mob play...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum explores the...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[: 1\\nReview: I'm a fan of the John Wick films...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>Based on the reviews provided, the action qual...</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[: 1\\nReview: I'm a fan of the John Wick films...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>In John Wick 3, one element that contributes t...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[: 13\\nReview: Following on from two delirious...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>The key themes and elements that make John Wic...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[: 14\\nReview: By now you know what to expect ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The latest John Wick film was a disappointment...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 18\\nReview: Ever since the original John Wi...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 23\\nReview: Rating 10/10\\nI was able to cat...   \n",
              "3  [: 19\\nReview: If you've seen the first John W...   \n",
              "4  [: 20\\nReview: After resolving his issues with...   \n",
              "5  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "6  [: 1\\nReview: I'm a fan of the John Wick films...   \n",
              "7  [: 1\\nReview: I'm a fan of the John Wick films...   \n",
              "8  [: 13\\nReview: Following on from two delirious...   \n",
              "9  [: 14\\nReview: By now you know what to expect ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The simple premise of John Wick, where the pro...   \n",
              "1  John Wick is loved by many because it features...   \n",
              "2  Keanu Reeves' performance in John Wick is high...   \n",
              "3  In the movie \"John Wick 2\", John Wick is calle...   \n",
              "4  In the plot of John Wick, the Russian mob play...   \n",
              "5  John Wick: Chapter 3 - Parabellum explores the...   \n",
              "6  Based on the reviews provided, the action qual...   \n",
              "7  In John Wick 3, one element that contributes t...   \n",
              "8  The key themes and elements that make John Wic...   \n",
              "9  The latest John Wick film was a disappointment...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  John Wick's premise is beautifully simple, foc...        1.000000   \n",
              "1  John Wick is a film series that has gained imm...        1.000000   \n",
              "2  Keanu's performance in John Wick stands out du...        0.000000   \n",
              "3  John Wick, a retired assassin known as the \"Bo...        0.000000   \n",
              "4  In John Wick, the Russian mob plays a signific...        0.666667   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...        0.666667   \n",
              "6  John Wick 3 is described as the best action mo...        0.250000   \n",
              "7  John Wick 3 is noted for creating something sp...        0.333333   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...        0.333333   \n",
              "9  The latest John Wick film was a bitter disappo...        0.000000   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.125000                    0.000000           1.000000  \n",
              "1               0.250000                    0.250000           0.833333  \n",
              "2               0.333333                    0.714286           1.000000  \n",
              "3               0.250000                    1.000000           1.000000  \n",
              "4               0.285714                    0.000000           1.000000  \n",
              "5               0.000000                    0.000000           1.000000  \n",
              "6               0.500000                    0.500000           1.000000  \n",
              "7               0.000000                    0.400000           1.000000  \n",
              "8               0.222222                    0.166667           1.000000  \n",
              "9               0.333333                    0.444444           0.583333  "
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method: Parent Document Retrieval\n",
            "context_recall [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.25, 0.3333333333333333, 0.3333333333333333, 0.0]\n",
            "context_precision [0.99999999995, 0.8333333332916666, 0.9999999999, 0.9999999999, 0.99999999995, 0.99999999995, 0.99999999995, 0.9999999999, 0.9999999999, 0.5833333333041666]\n",
            "noise_sensitivity_relevant [np.float64(0.0), np.float64(0.25), np.float64(0.7142857142857143), np.float64(1.0), np.float64(0.0), np.float64(0.0), np.float64(0.5), np.float64(0.4), np.float64(0.16666666666666666), np.float64(0.4444444444444444)]\n",
            "context_precision [0.99999999995, 0.8333333332916666, 0.9999999999, 0.9999999999, 0.99999999995, 0.99999999995, 0.99999999995, 0.9999999999, 0.9999999999, 0.5833333333041666]\n"
          ]
        }
      ],
      "source": [
        "print('Retrieval method: Parent Document Retrieval')\n",
        "print('context_recall', result['context_recall'])\n",
        "print('context_precision', result['context_precision'])\n",
        "print('noise_sensitivity_relevant', result['noise_sensitivity_relevant'])\n",
        "print('context_precision', result['context_precision'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric                      | Values                                                      |\n",
        "|-----------------------------|------------------------------------------------------------|\n",
        "| Retrieval method            | Parent Document Retrieval                                   |\n",
        "| context_recall              | [0.5, 0.5, 0.5, 0.6667, 0.25, 0.6667, 0.25, 0.5, 0.0, 0.5]  |\n",
        "| context_precision           | [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]        |\n",
        "| noise_sensitivity_relevant  | [0.6, 0.0, 0.6667, 0.2222, 0.625, 0.0, 0.0, 0.1429, 0.0, 0.2222] |\n",
        "| context_precision (repeat)   | [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ensemble Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = ensemble_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4918818c264b43b0b480acdf097f97f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[26]: TimeoutError()\n",
            "Exception raised in Job[34]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9667, 'context_entity_recall': 0.3167, 'noise_sensitivity_relevant': 0.3934, 'context_precision': 0.7591}"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The premise of John Wick contributes to its su...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.893759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 18\\nReview: Ever since the original John Wi...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>John Wick has gained immense popularity and lo...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.926840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu's performance in John Wick stands out in...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.855175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>John Wick faces significant challenges in the ...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.551474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>The Russian mob plays a significant role in th...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.986111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>In John Wick: Chapter 3, the character develop...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.542177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[: 1\\nReview: I'm a fan of the John Wick films...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>Based on the context provided, it appears that...</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.756443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[: 1\\nReview: I'm a fan of the John Wick films...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>The unique elements that contribute to the dis...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.705208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[: 13\\nReview: Following on from two delirious...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>The key themes and elements that make John Wic...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.730682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[: 14\\nReview: By now you know what to expect ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>I don't know the specific reasons why the late...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.642842</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 18\\nReview: Ever since the original John Wi...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 19\\nReview: If you've seen the first John W...   \n",
              "4  [: 20\\nReview: After resolving his issues with...   \n",
              "5  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "6  [: 1\\nReview: I'm a fan of the John Wick films...   \n",
              "7  [: 1\\nReview: I'm a fan of the John Wick films...   \n",
              "8  [: 13\\nReview: Following on from two delirious...   \n",
              "9  [: 14\\nReview: By now you know what to expect ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The premise of John Wick contributes to its su...   \n",
              "1  John Wick has gained immense popularity and lo...   \n",
              "2  Keanu's performance in John Wick stands out in...   \n",
              "3  John Wick faces significant challenges in the ...   \n",
              "4  The Russian mob plays a significant role in th...   \n",
              "5  In John Wick: Chapter 3, the character develop...   \n",
              "6  Based on the context provided, it appears that...   \n",
              "7  The unique elements that contribute to the dis...   \n",
              "8  The key themes and elements that make John Wic...   \n",
              "9  I don't know the specific reasons why the late...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  John Wick's premise is beautifully simple, foc...        1.000000   \n",
              "1  John Wick is a film series that has gained imm...        1.000000   \n",
              "2  Keanu's performance in John Wick stands out du...        1.000000   \n",
              "3  John Wick, a retired assassin known as the \"Bo...        1.000000   \n",
              "4  In John Wick, the Russian mob plays a signific...        1.000000   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...        0.666667   \n",
              "6  John Wick 3 is described as the best action mo...        1.000000   \n",
              "7  John Wick 3 is noted for creating something sp...        1.000000   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...        1.000000   \n",
              "9  The latest John Wick film was a bitter disappo...        1.000000   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.125000                    0.111111           0.893759  \n",
              "1               0.250000                    0.625000           0.926840  \n",
              "2               0.666667                    0.555556           0.855175  \n",
              "3               0.500000                    0.500000           0.551474  \n",
              "4               0.166667                    0.400000           0.986111  \n",
              "5               0.000000                    0.400000           0.542177  \n",
              "6               0.000000                         NaN           0.756443  \n",
              "7               0.666667                    0.555556           0.705208  \n",
              "8               0.125000                         NaN           0.730682  \n",
              "9               0.666667                    0.000000           0.642842  "
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method: Ensemble Retrieval\n",
            "context_recall [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]\n",
            "context_precision [0.8937585671349617, 0.9268402893341103, 0.855175380167606, 0.5514739228946162, 0.9861111110987848, 0.5421768707405539, 0.7564425770213568, 0.7052083333245183, 0.7306818181726847, 0.6428421259702533]\n",
            "noise_sensitivity_relevant [np.float64(0.1111111111111111), np.float64(0.625), np.float64(0.5555555555555556), np.float64(0.5), np.float64(0.4), np.float64(0.4), nan, np.float64(0.5555555555555556), nan, np.float64(0.0)]\n",
            "context_precision [0.8937585671349617, 0.9268402893341103, 0.855175380167606, 0.5514739228946162, 0.9861111110987848, 0.5421768707405539, 0.7564425770213568, 0.7052083333245183, 0.7306818181726847, 0.6428421259702533]\n"
          ]
        }
      ],
      "source": [
        "print('Retrieval method: Ensemble Retrieval')\n",
        "print('context_recall', result['context_recall'])\n",
        "print('context_precision', result['context_precision'])\n",
        "print('noise_sensitivity_relevant', result['noise_sensitivity_relevant'])\n",
        "print('context_precision', result['context_precision'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric                      | Values                                                      |\n",
        "|-----------------------------|------------------------------------------------------------|\n",
        "| Retrieval method            | Ensemble Retrieval                                         |\n",
        "| context_recall              | [1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0]       |\n",
        "| context_precision           | [0.8691, 0.5121, 0.9206, 0.9060, 0.8976, 0.8123, 1.0, 0.7528, 0.8304, 1.0] |\n",
        "| noise_sensitivity_relevant  | [0.4286, 0.0, 0.25, 0.0, 0.0, 0.125, 0.2143, 0.6667, 0.5, 0.25] |\n",
        "| context_precision (repeat)   | [0.8691, 0.5121, 0.9206, 0.9060, 0.8976, 0.8123, 1.0, 0.7528, 0.8304, 1.0] |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Semantic Chunking Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "for test_row in dataset:\n",
        "  response = semantic_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98c7dcca8ece49ba90e5c20192084539",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.8667, 'context_entity_recall': 0.2375, 'noise_sensitivity_relevant': 0.5083, 'context_precision': 0.8191}"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How does John Wick's premise contribute to its...</td>\n",
              "      <td>[John Wick (Reeves) is out to seek revenge on ...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>The premise of John Wick, where the main chara...</td>\n",
              "      <td>John Wick's premise is beautifully simple, foc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.962654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So like, what is the deal with John Wick and w...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The John Wick movies are loved for their exper...</td>\n",
              "      <td>John Wick is a film series that has gained imm...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu's performance in John wick st...</td>\n",
              "      <td>[this movie so amazing !! One of the best acti...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu's performance in \"John Wick\" stands out ...</td>\n",
              "      <td>Keanu's performance in John Wick stands out du...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.982143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick 2,\" John Wick is invol...</td>\n",
              "      <td>John Wick, a retired assassin known as the \"Bo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.648810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What role does the Russian mob play in the plo...</td>\n",
              "      <td>[A few days later some thugs, led by the son o...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>The Russian mob plays a significant role in th...</td>\n",
              "      <td>In John Wick, the Russian mob plays a signific...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.961735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In what ways does John Wick's character develo...</td>\n",
              "      <td>[: 24\\nReview: John Wick: Chapter 3 - Parabell...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick's character development in Chapter 3...</td>\n",
              "      <td>In John Wick: Chapter 3 - Parabellum, John’s c...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the action quality in John Wick 3 com...</td>\n",
              "      <td>[: 19\\nReview: The inevitable third chapter of...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>Based on the context provided, the action qual...</td>\n",
              "      <td>John Wick 3 is described as the best action mo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.855159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What elements contribute to the uniqueness of ...</td>\n",
              "      <td>[: 5\\nReview: The first John Wick film was spe...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 22\\nReview: Lets contemplate abo...</td>\n",
              "      <td>I don't know the specifics of John Wick 3 comp...</td>\n",
              "      <td>John Wick 3 is noted for creating something sp...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.468254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the key themes and elements that make...</td>\n",
              "      <td>[: 13\\nReview: Following on from two delirious...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 24\\nReview: John Wick: Chapter 3...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum is praised f...</td>\n",
              "      <td>John Wick: Chapter 3 - Parabellum stands out i...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.895139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why was the latest John Wick film a disappoint...</td>\n",
              "      <td>[But we'll get to that in a bit...! Anyway, I ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>I don't know, but based on the provided contex...</td>\n",
              "      <td>The latest John Wick film was a bitter disappo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How does John Wick's premise contribute to its...   \n",
              "1  So like, what is the deal with John Wick and w...   \n",
              "2  What makes Keanu's performance in John wick st...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  What role does the Russian mob play in the plo...   \n",
              "5  In what ways does John Wick's character develo...   \n",
              "6  How does the action quality in John Wick 3 com...   \n",
              "7  What elements contribute to the uniqueness of ...   \n",
              "8  What are the key themes and elements that make...   \n",
              "9  Why was the latest John Wick film a disappoint...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [John Wick (Reeves) is out to seek revenge on ...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [this movie so amazing !! One of the best acti...   \n",
              "3  [: 19\\nReview: If you've seen the first John W...   \n",
              "4  [A few days later some thugs, led by the son o...   \n",
              "5  [: 24\\nReview: John Wick: Chapter 3 - Parabell...   \n",
              "6  [: 19\\nReview: The inevitable third chapter of...   \n",
              "7  [: 5\\nReview: The first John Wick film was spe...   \n",
              "8  [: 13\\nReview: Following on from two delirious...   \n",
              "9  [But we'll get to that in a bit...! Anyway, I ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "6  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "7  [<1-hop>\\n\\n: 22\\nReview: Lets contemplate abo...   \n",
              "8  [<1-hop>\\n\\n: 24\\nReview: John Wick: Chapter 3...   \n",
              "9  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "\n",
              "                                            response  \\\n",
              "0  The premise of John Wick, where the main chara...   \n",
              "1  The John Wick movies are loved for their exper...   \n",
              "2  Keanu's performance in \"John Wick\" stands out ...   \n",
              "3  In the movie \"John Wick 2,\" John Wick is invol...   \n",
              "4  The Russian mob plays a significant role in th...   \n",
              "5  John Wick's character development in Chapter 3...   \n",
              "6  Based on the context provided, the action qual...   \n",
              "7  I don't know the specifics of John Wick 3 comp...   \n",
              "8  John Wick: Chapter 3 - Parabellum is praised f...   \n",
              "9  I don't know, but based on the provided contex...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  John Wick's premise is beautifully simple, foc...        1.000000   \n",
              "1  John Wick is a film series that has gained imm...        1.000000   \n",
              "2  Keanu's performance in John Wick stands out du...        0.333333   \n",
              "3  John Wick, a retired assassin known as the \"Bo...        1.000000   \n",
              "4  In John Wick, the Russian mob plays a signific...        1.000000   \n",
              "5  In John Wick: Chapter 3 - Parabellum, John’s c...        0.666667   \n",
              "6  John Wick 3 is described as the best action mo...        1.000000   \n",
              "7  John Wick 3 is noted for creating something sp...        0.666667   \n",
              "8  John Wick: Chapter 3 - Parabellum stands out i...        1.000000   \n",
              "9  The latest John Wick film was a bitter disappo...        1.000000   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.125000                    0.083333           0.962654  \n",
              "1               0.250000                    0.285714           1.000000  \n",
              "2               0.333333                    0.600000           0.982143  \n",
              "3               0.500000                    0.666667           0.648810  \n",
              "4               0.166667                    0.600000           0.961735  \n",
              "5               0.000000                    0.142857           0.750000  \n",
              "6               0.000000                    0.454545           0.855159  \n",
              "7               0.000000                    1.000000           0.468254  \n",
              "8               0.000000                    0.500000           0.895139  \n",
              "9               1.000000                    0.750000           0.666667  "
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method: Semantic Chunking Retrieval\n",
            "context_recall [1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0]\n",
            "context_precision [0.9626543209769582, 0.99999999999, 0.9821428571288265, 0.6488095237933036, 0.9617346938638119, 0.7499999999625, 0.8551587301444776, 0.4682539682422619, 0.8951388888776997, 0.66666666665]\n",
            "noise_sensitivity_relevant [np.float64(0.08333333333333333), np.float64(0.2857142857142857), np.float64(0.6), np.float64(0.6666666666666666), np.float64(0.6), np.float64(0.14285714285714285), np.float64(0.45454545454545453), np.float64(1.0), np.float64(0.5), np.float64(0.75)]\n",
            "context_precision [0.9626543209769582, 0.99999999999, 0.9821428571288265, 0.6488095237933036, 0.9617346938638119, 0.7499999999625, 0.8551587301444776, 0.4682539682422619, 0.8951388888776997, 0.66666666665]\n"
          ]
        }
      ],
      "source": [
        "print('Retrieval method: Semantic Chunking Retrieval')\n",
        "print('context_recall', result['context_recall'])\n",
        "print('context_precision', result['context_precision'])\n",
        "print('noise_sensitivity_relevant', result['noise_sensitivity_relevant'])\n",
        "print('context_precision', result['context_precision'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Metric                      | Values                                                      |\n",
        "|-----------------------------|------------------------------------------------------------|\n",
        "| Retrieval method            | Semantic Chunking Retrieval                                 |\n",
        "| context_recall              | [1.0, 1.0, 1.0, 0.6667, 0.75, 0.3333, 1.0, 1.0, 1.0, 1.0]  |\n",
        "| context_precision           | [0.8486, 0.3873, 0.9861, 0.7823, 0.8968, 0.9129, 0.9468, 0.9472, 1.0, 0.8951] |\n",
        "| noise_sensitivity_relevant  | [0.6, 0.0, 0.3077, 0.0, 0.0, 1.0, 0.1667, 0.4667, 0.2, 0.1111] |\n",
        "| context_precision (repeat)   | [0.8486, 0.3873, 0.9861, 0.7823, 0.8968, 0.9129, 0.9468, 0.9472, 1.0, 0.8951] |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final retriever comparison\n",
        "\n",
        "| Retrieval Method               | Context Recall | Context Entity Recall | Noise Sensitivity Relevant | Context Precision |\n",
        "|--------------------------------|----------------|-----------------------|---------------------------|------------------|\n",
        "| Naive Retrieval               | 0.9667        | 0.2643               | 0.4913                   | 0.8654          |\n",
        "| BM25 Retrieval                | 0.8083        | 0.2994               | 0.0950                   | 0.6583          |\n",
        "| Contextual Compression Retrieval | 0.6500        | 0.2476               | 0.3823                   | 0.9333          |\n",
        "| Multi-Query Retrieval         | 0.9167        | 0.2823               | 0.5841                   | 0.7933          |\n",
        "| Parent-Document Retrieval     | 0.4250        | 0.2300               | 0.3475                   | 0.9417          |\n",
        "| Ensemble Retrieval            | 0.9667        | 0.3167               | 0.3934                   | 0.7591          |\n",
        "| Semantic Chunking Retrieval    | 0.8667        | 0.2375               | 0.5083                   | 0.8191          |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison summarization\n",
        "\n",
        "Based on the evaluation metrics and the John Wick review dataset, here's an analysis of the best retrieval methods:\n",
        "\n",
        "The Ensemble Retriever appears to be the most well-rounded choice for this particular dataset. Here's why:\n",
        "\n",
        "1. **Highest Context Entity Recall (0.3167)**: This is crucial for a movie review dataset where specific entities (characters, actors, locations) are important for providing accurate context. The Ensemble Retriever's ability to capture these entities better than other methods makes it particularly suitable for movie-related queries.\n",
        "\n",
        "2. **Strong Context Recall (0.9667)**: It ties with Naive Retrieval for the highest recall, meaning it successfully retrieves most relevant information from the reviews. This is essential when users ask broad questions about plot points or general reception.\n",
        "\n",
        "3. **Balanced Noise Sensitivity (0.3934)**: While not the best, it shows good resistance to noise while maintaining high recall, suggesting it can handle the varied writing styles and subjective nature of movie reviews.\n",
        "\n",
        "4. **Reasonable Context Precision (0.7591)**: Though not the highest, it maintains good precision while achieving high recall, striking a practical balance for this use case.\n",
        "\n",
        "While Parent-Document Retrieval shows the highest precision (0.9417) and Contextual Compression shows excellent precision (0.9333), their significantly lower recall scores (0.4250 and 0.6500 respectively) make them less suitable for a review dataset where comprehensive coverage of opinions and plot points is important. The Ensemble Retriever's ability to combine the strengths of multiple approaches while mitigating their individual weaknesses makes it the most effective choice for handling diverse queries about movie reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from uuid import uuid4\n",
        "from langsmith import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = Client()\n",
        "client = Client(api_key=os.environ[\"LANGCHAIN_API_KEY\"])\n",
        "dataset_name = \"JW Retrieval Methods\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Evaluating JW docs (Naive, BM25, Contextual Compression, Multi-Query, Parent-Document, Ensemble, Semantic Chunking)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How does John Wick's premise contribute to its success as an action movie?\n",
            "John Wick's premise is beautifully simple, focusing on revenge for something the protagonist loves, which allows the film to deliver awesome action, stylish stunts, and kinetic chaos, all tied together by a relatable hero. This simplicity is key to its success, especially compared to more convoluted action films.\n",
            "[\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\"]\n",
            "114fed35-8bbe-4294-b7e8-7918690cc88a\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "    print(data_row[1][\"user_input\"])\n",
        "    print(data_row[1][\"reference\"])\n",
        "    print(data_row[1][\"reference_contexts\"])\n",
        "    print(langsmith_dataset.id)\n",
        "    print(\"--------------------------------\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator ,evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "       \"prediction\": (run.outputs[\"response\"].content if hasattr(run.outputs[\"response\"], \"content\") \n",
        "                  else str(run.outputs[\"response\"]) if hasattr(run.outputs, \"response\") \n",
        "                  else run.outputs.content if hasattr(run.outputs, \"content\") \n",
        "                  else str(run.outputs)),\n",
        "        \"reference\": str(example.outputs[\"answer\"]),\n",
        "        \"input\": str(example.inputs[\"question\"]) \n",
        "    }\n",
        ")\n",
        "\n",
        "relevance_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"relevance\": \"Does the response directly address the user's question in a relevant manner?\"\n",
        "        },\n",
        "        \"llm\": eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": (run.outputs[\"response\"].content if hasattr(run.outputs[\"response\"], \"content\") \n",
        "                        else str(run.outputs[\"response\"]) if hasattr(run.outputs, \"response\") \n",
        "                        else run.outputs.content if hasattr(run.outputs, \"content\") \n",
        "                        else str(run.outputs)),\n",
        "        \"reference\": str(example.outputs[\"answer\"]),\n",
        "        \"input\": str(example.inputs[\"question\"])\n",
        "    }\n",
        ")\n",
        "\n",
        "grounded_relevance_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"grounded_relevance\": \"Is the response factually accurate and grounded based on the reference answer?\"\n",
        "        },\n",
        "        \"llm\": eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": (run.outputs[\"response\"].content if hasattr(run.outputs[\"response\"], \"content\") \n",
        "                        else str(run.outputs[\"response\"]) if hasattr(run.outputs, \"response\") \n",
        "                        else run.outputs.content if hasattr(run.outputs, \"content\") \n",
        "                        else str(run.outputs)),\n",
        "        \"reference\": str(example.outputs[\"answer\"]),\n",
        "        \"input\": str(example.inputs[\"question\"])\n",
        "    }\n",
        ")\n",
        "\n",
        "retrieval_quality_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"retrieval_quality\": \"How well does the response leverage retrieved documents to answer the question?\"\n",
        "        },\n",
        "        \"llm\": eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": (run.outputs[\"response\"].content if hasattr(run.outputs[\"response\"], \"content\") \n",
        "                        else str(run.outputs[\"response\"]) if hasattr(run.outputs, \"response\") \n",
        "                        else run.outputs.content if hasattr(run.outputs, \"content\") \n",
        "                        else str(run.outputs)),\n",
        "        \"reference\": str(example.outputs[\"answer\"]),\n",
        "        \"input\": str(example.inputs[\"question\"])\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'naive_retrieval_chain-dbee2207' at:\n",
            "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/114fed35-8bbe-4294-b7e8-7918690cc88a/compare?selectedSessions=72fa1aeb-ac96-4379-981d-db82438072d1\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0424b87c5ae14cd2b62487d9a206dd1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5270a1ce-e62b-4a21-b6f1-8868eb493d14: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dc4af0d0-1db7-4f5e-9fe7-a3bb0ce62fba: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5fd6558b-5386-41f2-bc5b-02cde29b90b0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7355257d-307a-4b6b-a5b0-7b4730b9a3d4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 192bdc77-eab8-47fd-9bf7-725163785b71: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1d427710-282e-472a-a9de-2e55e4207b4a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7ca82581-3434-4e7d-8a2b-b39c9d8f6a15: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2213599a-8239-4e70-81ea-5beaf383890a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 918f4777-c151-483e-82e1-b959a0cb43c5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5b6b0b51-83ac-4e16-9501-88d3e706de76: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "naive_retrieval_evaluation = evaluate(\n",
        "    naive_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        relevance_evaluator,\n",
        "        grounded_relevance_evaluator,\n",
        "        retrieval_quality_evaluator \n",
        "    ],\n",
        "    experiment_prefix=\"naive_retrieval_chain\",\n",
        "    metadata={\"revision_id\": \"naive_retrieval_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'bm25_retrieval_evaluation-f7abf53b' at:\n",
            "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/114fed35-8bbe-4294-b7e8-7918690cc88a/compare?selectedSessions=5edb61c4-4deb-4367-91ef-74d12bd538b9\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dd352e29dee4a42b12bc731fa9a2f55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1fe9f776-b4e3-40e8-9ad6-4fa24372d94e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5a8b7e73-f334-4948-af0b-c58774a0bc85: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b5420f57-a15e-4038-b254-a7000cc7c95b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0214d2ef-3ff8-400d-9d22-33edab9200ac: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 28d8aaa0-2bc7-47b7-8d49-699b162d3c0b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0667f899-0bbb-4738-a35e-30461de70f51: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1effd736-b9c5-4356-a2c8-f7273a6a051d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c1a65e82-aed3-471c-ac63-274f4c02a275: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e5638315-aa23-47a1-a0a0-0a2eb80a3e3d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2a62f449-fcda-4cc0-b478-62617d246d1d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bm25_retrieval_evaluation = evaluate(\n",
        "    bm25_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        relevance_evaluator,\n",
        "        grounded_relevance_evaluator,\n",
        "        retrieval_quality_evaluator \n",
        "    ],\n",
        "    experiment_prefix=\"bm25_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"bm25_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the evaluation results for experiment: 'crushing-rule-24' at:\n",
        "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/b80af506-cfd1-4a4e-a21c-a4026acb5854/compare?selectedSessions=264ee670-6bef-427f-a6be-14cf01e00c64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'contextual_compression_retrieval_evaluation-3fb2da7e' at:\n",
            "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/114fed35-8bbe-4294-b7e8-7918690cc88a/compare?selectedSessions=3b8958a9-038e-4393-9a79-2d9046b868d7\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd20be4a7e1c4059acdceb7426257acd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d321ac49-6031-4187-9186-cd02baf30729: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 589d9f4a-3f6b-41b2-b553-b62e18b0a223: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a300b9b5-cf86-4f67-b733-36ffdfc491ca: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d81527f7-9fff-4682-8e44-e26b58c17b10: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f6f21693-d4c1-431b-81b1-e8b643ea24eb: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0aefeb7a-53b2-45fa-8624-d941fed57d09: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1a931e1a-acd6-4380-bccb-88a2f7db30a1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6846b319-7017-4903-b6a4-bcf2842200a0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7b1636c8-9332-4785-9bc4-dea2c91a39fe: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ac6a8f8c-c3aa-4878-9028-afb9a5d79862: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "contextual_compression_retrieval_evaluation = evaluate(\n",
        "    contextual_compression_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        relevance_evaluator,\n",
        "        grounded_relevance_evaluator,\n",
        "        retrieval_quality_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"contextual_compression_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"contextual_compression_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the evaluation results for experiment: 'dependable-watch-31' at:\n",
        "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/b80af506-cfd1-4a4e-a21c-a4026acb5854/compare?selectedSessions=a29faba0-1232-4dcd-b986-7fdb8145e83b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'multi_query_retrieval_evaluation-67cd3e0d' at:\n",
            "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/114fed35-8bbe-4294-b7e8-7918690cc88a/compare?selectedSessions=036f077e-f330-4fce-97af-41e9a999a8d9\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33e1120f6f4c4616b6e30202f7032d32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d502ec66-6724-41d1-815d-348e122e1cb3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d5b97756-bd21-4730-a1ca-ca27bf007ced: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9fff47ae-9c2c-44db-9c33-a3974f77f820: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 398a5f17-9b69-4376-be42-c8be7fc31261: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 026a9333-0ff7-4cdc-b26b-7957e3774538: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e294c6c6-0f45-4c33-9cde-0fe72019f7f6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 433aaa43-217b-41a6-89b3-489bf29fbff7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c3e1a12f-af83-4d2b-8095-82f40b838103: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dc19916a-098b-471a-91ab-49f7c6ddef7d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2d0752e7-4944-4234-bb9d-ab35a657c0ee: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "multi_query_retrieval_evaluation = evaluate(\n",
        "    multi_query_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        relevance_evaluator,\n",
        "        grounded_relevance_evaluator,\n",
        "        retrieval_quality_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"multi_query_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"multi_query_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the evaluation results for experiment: 'proper-competition-5' at:\n",
        "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/b80af506-cfd1-4a4e-a21c-a4026acb5854/compare?selectedSessions=a718c41c-766f-41f4-95df-8a12d241274d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'parent_document_retrieval_evaluation-54a6308b' at:\n",
            "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/114fed35-8bbe-4294-b7e8-7918690cc88a/compare?selectedSessions=566f664f-12cc-4692-b80f-7eaf2c7660dc\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd01f2ee29564c00b0f793d5d9ed1877",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cbd23a1b-ab46-4beb-abd4-51aa0d5273b5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c5a78977-84c3-4e78-b448-5931b7164af9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 86ffb7b8-ffeb-4dc9-aa09-38ffb48c5d03: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b330f8b2-3655-4b96-81f3-d4cbbc4101b6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1a5729a6-e9aa-40e7-b70b-c5f922d52ff1: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c6b6d5e4-387f-490a-8ae0-d2f6d3787f53: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b9e01777-0841-46fa-9567-7a750f40e074: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 62c22f62-e4af-45cb-992c-5abf91aaad81: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ef72b3ca-b629-404b-b757-cf29c0e636d7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 389ccb2d-74d2-4474-92ff-c82df6b84c1b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parent_document_retrieval_evaluation = evaluate(\n",
        "    parent_document_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        relevance_evaluator,\n",
        "        grounded_relevance_evaluator,\n",
        "        retrieval_quality_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"parent_document_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"parent_document_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the evaluation results for experiment: 'long-library-86' at:\n",
        "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/b80af506-cfd1-4a4e-a21c-a4026acb5854/compare?selectedSessions=7323a50a-f3b2-48d5-9c47-6a67b9b832da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'ensemble_retrieval_evaluation-27dbe483' at:\n",
            "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/114fed35-8bbe-4294-b7e8-7918690cc88a/compare?selectedSessions=c084e2c3-1b5a-455b-87a5-f180f9ddd587\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81e2fa84579b468fbe75894bae7157b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 05cd2bde-0fcc-41e5-8808-8d8243fa4b8e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 69f51545-ebe9-4983-802b-abbbc62838de: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8bc36d7d-15c4-4848-a744-73206e804f85: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1a9d23fb-e9bb-4dda-827b-73a9066427fc: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0428f0ee-3ebe-4076-826b-efc8b0b59307: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5c0159ae-fca7-48b4-a10d-749c327efbbd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c9c91c77-902e-478d-a724-6b1b5c3a4c08: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4fa2acd7-985c-4160-8b06-2ea7471a09b9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 742b4f00-a13d-4851-a683-292db97fd864: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7314b603-9be4-460a-9fe6-55dbe98096a8: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ensemble_retrieval_evaluation = evaluate(\n",
        "    ensemble_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        relevance_evaluator,\n",
        "        grounded_relevance_evaluator,\n",
        "        retrieval_quality_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"ensemble_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"ensemble_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the evaluation results for experiment: 'bold-value-12' at:\n",
        "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/b80af506-cfd1-4a4e-a21c-a4026acb5854/compare?selectedSessions=1d221440-72e3-4489-8f11-fcdd6de957e2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'semantic_retrieval_evaluation-7ef7849c' at:\n",
            "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/114fed35-8bbe-4294-b7e8-7918690cc88a/compare?selectedSessions=58cbfded-55f3-4bfe-b6aa-957a97919059\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c79440688b5049d5aa092d22a496a78e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2b7141ef-5da5-47cf-8a78-ddfa42aa51ac: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 2c509db6-2cf5-4ba2-9cec-f2f4a874a156: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 1158dbf4-231e-4e49-ad07-cb64ae297fa5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e541e278-9a25-4c21-9e60-e80ff9e4fcc4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f139657-3560-42af-bb9b-1e42ea47282b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 36260e9a-23c2-4f90-9179-7c76e6c51f2b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c2721503-95e7-4080-903a-0af9319b9103: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 55038fd9-ca6f-4df1-8573-26699ada5fc7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cd8c72e6-68d2-459a-9de2-626eed4a7910: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 38468842-6e7b-4919-9b22-792971acc7f4: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/mnt/c/Users/email/Desktop/bootcamp/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fd5d46359d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fd5d46377d0>, root_client=<openai.OpenAI object at 0x7fd5d46cdd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fd5d46ce0d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "semantic_retrieval_evaluation = evaluate(\n",
        "    semantic_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        relevance_evaluator,\n",
        "        grounded_relevance_evaluator,\n",
        "        retrieval_quality_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"semantic_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"semantic_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the evaluation results for experiment: 'flowery-dinner-94' at:\n",
        "https://smith.langchain.com/o/5893d499-6998-4f44-a84d-2fcf6d99ac9b/datasets/b80af506-cfd1-4a4e-a21c-a4026acb5854/compare?selectedSessions=383a032e-b9ca-4d97-a459-719554d1119b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis Report: Best Retrieval Method Evaluation\n",
        "\n",
        "#### Overview\n",
        "The analysis compares different retrieval methods based on three key factors: **Cost**, **Latency**, and **Performance**. The goal is to identify the best retrieval method for the given data.\n",
        "\n",
        "#### Evaluation Criteria\n",
        "1. **Cost**: Measured indirectly by P50 and P99 latencies.\n",
        "2. **Latency**: Lower latency indicates faster response time.\n",
        "3. **Performance**: Evaluated through Grounded Relevance, Helpfulness, Relevance, Retrieval Quality, and Final Retriever Comparison metrics.\n",
        "\n",
        "#### Summary of Results\n",
        "| Method                         | P50 Latency | P99 Latency | Grounded Relevance | Helpfulness | Relevance | Retrieval Quality | Context Recall | Context Precision | Noise Sensitivity | Final Score |\n",
        "|--------------------------------|-------------|-------------|-------------------|-------------|-----------|------------------|---------------|----------------|----------------|-------------|\n",
        "| Naive Retrieval               | 1.96s      | 2.44s      | 5               | 5           | 7        | 3                | **0.9667**   | 0.8654        | 0.4913        | 7.5        |\n",
        "| BM25 Retrieval                | **1.09s**  | **1.53s**  | 6               | 4           | 5        | 5                | 0.8083       | 0.6583        | **0.0950**    | 6          |\n",
        "| Contextual Compression        | 2.43s      | 5.21s      | 6               | 4           | 7        | 3                | 0.6500       | **0.9333**    | 0.3823        | 6.5        |\n",
        "| Multi-Query Retrieval         | 3.42s      | 6.35s      | 6               | 4           | 6        | 4                | 0.9167       | 0.7933        | 0.5841        | 6.5        |\n",
        "| Parent-Document Retrieval     | 2.14s      | 3.01s      | 7               | 3           | 7        | 3                | 0.4250       | **0.9417**    | 0.3475        | 6.25       |\n",
        "| Ensemble Retrieval            | 6.02s      | 9.84s      | 6               | 4           | 5        | 9                | **0.9667**   | 0.7591        | 0.3934        | **7.75**   |\n",
        "| Semantic Chunking Retrieval    | 1.92s      | 4.19s      | 8               | 2           | 6        | 4                | 0.8667       | 0.8191        | 0.5083        | 7          |\n",
        "\n",
        "#### Key Observations\n",
        "- **Naive Retrieval** has the best context recall but higher noise sensitivity and lower precision.\n",
        "- **BM25 Retrieval** is the fastest method with the lowest noise sensitivity but sacrifices context recall and precision.\n",
        "- **Ensemble Retrieval** combines the best context recall with the highest retrieval quality but suffers from high latency.\n",
        "- **Semantic Chunking Retrieval** balances performance and latency, making it the most consistent method.\n",
        "- **Contextual Compression** achieves the highest precision but at the cost of recall.\n",
        "\n",
        "#### Conclusion\n",
        "The **Semantic Chunking Retrieval** method emerges as the best option based on the following:\n",
        "- Balanced performance across all evaluation metrics.\n",
        "- Moderate latency and cost.\n",
        "- Consistent retrieval quality.\n",
        "\n",
        "However, if **maximum recall** is required and **latency is not a concern**, **Ensemble Retrieval** is the best option.\n",
        "\n",
        "If **speed and cost-efficiency** are the highest priority, **BM25 Retrieval** is the ideal choice.\n",
        "\n",
        "Let me know if you would like to generate visualizations or further fine-tune the recommendations.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
